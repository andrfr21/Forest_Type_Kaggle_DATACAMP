{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d57fa4c-f006-45e0-a7bb-d3ce341d51c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6681548\ttest: 0.6431878\tbest: 0.6431878 (0)\ttotal: 324ms\tremaining: 10m 46s\n",
      "200:\tlearn: 0.8843419\ttest: 0.8230820\tbest: 0.8230820 (200)\ttotal: 23.6s\tremaining: 3m 31s\n",
      "400:\tlearn: 0.9383267\ttest: 0.8475529\tbest: 0.8475529 (392)\ttotal: 45.8s\tremaining: 3m 2s\n",
      "600:\tlearn: 0.9638724\ttest: 0.8594577\tbest: 0.8594577 (598)\ttotal: 1m 7s\tremaining: 2m 38s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.8601190476\n",
      "bestIteration = 614\n",
      "\n",
      "Shrink model to first 615 iterations.\n",
      "Accuracy: 0.8601190476190477\n",
      "\n",
      "Top 10 features:\n",
      " Elev_minus_VertHydro                  17.399888\n",
      "Elevation                             15.365929\n",
      "Aspect_cos                             5.930192\n",
      "Hydro_Dist                             5.555803\n",
      "Wilderness_Area3                       4.311700\n",
      "Vertical_Distance_To_Hydrology         4.285341\n",
      "Horizontal_Distance_To_Fire_Points     3.863735\n",
      "Hillshade_9am                          3.564890\n",
      "Hydro_Road                             3.560580\n",
      "Horizontal_Distance_To_Roadways        3.554467\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ===== 0) Imports =====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# ===== 1) Load =====\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# y / X de base (on exclut Id et Cover_Type)\n",
    "y = train[\"Cover_Type\"].astype(int)\n",
    "X = train.drop(columns=[\"Cover_Type\", \"Id\"])\n",
    "\n",
    "# ===== 2) Feature engineering minimal mais utile =====\n",
    "# Aspect est angulaire → sin/cos\n",
    "X[\"Aspect_sin\"] = np.sin(np.deg2rad(train[\"Aspect\"]))\n",
    "X[\"Aspect_cos\"] = np.cos(np.deg2rad(train[\"Aspect\"]))\n",
    "\n",
    "# Distance combinée à l’hydrologie\n",
    "X[\"Hydro_Dist\"] = np.hypot(train[\"Horizontal_Distance_To_Hydrology\"],\n",
    "                           train[\"Vertical_Distance_To_Hydrology\"])\n",
    "\n",
    "# Le plus proche entre route et feu\n",
    "X[\"Near_RoadOrFire\"] = np.minimum(train[\"Horizontal_Distance_To_Roadways\"],\n",
    "                                  train[\"Horizontal_Distance_To_Fire_Points\"])\n",
    "\n",
    "# Différence d’altitude vs hydrologie (souvent très discriminant)\n",
    "X[\"Elev_minus_VertHydro\"] = train[\"Elevation\"] - train[\"Vertical_Distance_To_Hydrology\"]\n",
    "\n",
    "# Quelques sommes de distances classiques sur ce dataset\n",
    "X[\"Road_Fire\"] = train[\"Horizontal_Distance_To_Roadways\"] + train[\"Horizontal_Distance_To_Fire_Points\"]\n",
    "X[\"Hydro_Road\"] = train[\"Horizontal_Distance_To_Hydrology\"] + train[\"Horizontal_Distance_To_Roadways\"]\n",
    "X[\"Hydro_Fire\"] = train[\"Horizontal_Distance_To_Hydrology\"] + train[\"Horizontal_Distance_To_Fire_Points\"]\n",
    "\n",
    "# NB: Pas besoin de reconstruire Soil_Type/Wilderness en une colonne.\n",
    "# Les 40+4 dummies 0/1 peuvent rester telles quelles — CatBoost s’en sort très bien.\n",
    "\n",
    "# ===== 3) Split =====\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ===== 4) Modèle (simple et efficace) =====\n",
    "model = CatBoostClassifier(\n",
    "    iterations=2000,\n",
    "    depth=10,\n",
    "    learning_rate=0.05,\n",
    "    l2_leaf_reg=3,\n",
    "    loss_function=\"MultiClass\",\n",
    "    eval_metric=\"Accuracy\",\n",
    "    random_seed=42,\n",
    "    # task_type=\"GPU\",  # <- décommente si tu as un GPU, c'est (beaucoup) plus rapide\n",
    "    verbose=200\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50)\n",
    "\n",
    "# ===== 5) Évaluation =====\n",
    "y_pred = model.predict(X_val).ravel().astype(int)\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "\n",
    "# (Option) Top features pour comprendre ce qui aide\n",
    "imp = pd.Series(model.get_feature_importance(), index=X.columns).sort_values(ascending=False)\n",
    "print(\"\\nTop 10 features:\\n\", imp.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a82e7bc7-c735-49de-97c5-3b65fa43ea6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6292989\ttest: 0.6283069\tbest: 0.6283069 (0)\ttotal: 138ms\tremaining: 6m 53s\n",
      "200:\tlearn: 0.8655754\ttest: 0.8078704\tbest: 0.8085317 (194)\ttotal: 22.7s\tremaining: 5m 15s\n",
      "400:\tlearn: 0.9162533\ttest: 0.8376323\tbest: 0.8376323 (374)\ttotal: 45s\tremaining: 4m 51s\n",
      "600:\tlearn: 0.9429563\ttest: 0.8465608\tbest: 0.8472222 (592)\ttotal: 1m 7s\tremaining: 4m 28s\n",
      "800:\tlearn: 0.9617229\ttest: 0.8544974\tbest: 0.8544974 (797)\ttotal: 1m 29s\tremaining: 4m 6s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.8551587302\n",
      "bestIteration = 837\n",
      "\n",
      "Shrink model to first 838 iterations.\n",
      "Accuracy: 0.8551587301587301\n",
      "\n",
      "Top 12 features:\n",
      " Soil_Type                             16.057298\n",
      "Elev_minus_VertHydro                  13.194269\n",
      "Elevation                             11.005252\n",
      "Wilderness_Area                       10.520790\n",
      "Horizontal_Distance_To_Roadways        7.306233\n",
      "Horizontal_Distance_To_Fire_Points     6.625909\n",
      "Aspect_cos                             4.604151\n",
      "Near_RoadOrFire                        4.568377\n",
      "Hydro_Dist                             4.189255\n",
      "Abs_VertHydro                          3.285842\n",
      "Horizontal_Distance_To_Hydrology       2.837742\n",
      "Hillshade_9am                          2.366458\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# ===== 1) Load =====\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# ===== 2) Rebuild categorical Soil_Type & Wilderness_Area =====\n",
    "soil_cols = [c for c in df.columns if c.startswith(\"Soil_Type\")]\n",
    "wild_cols = [c for c in df.columns if c.startswith(\"Wilderness_Area\")]\n",
    "\n",
    "df[\"Soil_Type\"] = df[soil_cols].idxmax(axis=1).str.extract(r\"(\\d+)\").astype(int).astype(\"category\")\n",
    "df[\"Wilderness_Area\"] = df[wild_cols].idxmax(axis=1).str.extract(r\"(\\d+)\").astype(int).astype(\"category\")\n",
    "\n",
    "# Option: drop dummies (on garde le dataset compact)\n",
    "df = df.drop(columns=soil_cols + wild_cols)\n",
    "\n",
    "# ===== 3) Features simples mais payantes =====\n",
    "df[\"Aspect_sin\"] = np.sin(np.deg2rad(df[\"Aspect\"]))\n",
    "df[\"Aspect_cos\"] = np.cos(np.deg2rad(df[\"Aspect\"]))\n",
    "df[\"Hydro_Dist\"] = np.hypot(df[\"Horizontal_Distance_To_Hydrology\"], df[\"Vertical_Distance_To_Hydrology\"])\n",
    "df[\"Near_RoadOrFire\"] = np.minimum(df[\"Horizontal_Distance_To_Roadways\"], df[\"Horizontal_Distance_To_Fire_Points\"])\n",
    "df[\"Elev_minus_VertHydro\"] = df[\"Elevation\"] - df[\"Vertical_Distance_To_Hydrology\"]\n",
    "\n",
    "# 3 nouvelles\n",
    "df[\"Abs_VertHydro\"] = df[\"Vertical_Distance_To_Hydrology\"].abs()\n",
    "df[\"Hillshade_mean\"] = df[[\"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\"]].mean(axis=1)\n",
    "df[\"Hillshade_range\"] = df[[\"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\"]].max(axis=1) - \\\n",
    "                        df[[\"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\"]].min(axis=1)\n",
    "\n",
    "# ===== 4) Split =====\n",
    "y = df[\"Cover_Type\"].astype(int)\n",
    "X = df.drop(columns=[\"Cover_Type\", \"Id\"])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# indices des colonnes catégorielles pour CatBoost\n",
    "cat_features = [X.columns.get_loc(\"Soil_Type\"), X.columns.get_loc(\"Wilderness_Area\")]\n",
    "\n",
    "# ===== 5) CatBoost (réglages simples & efficaces) =====\n",
    "model = CatBoostClassifier(\n",
    "    iterations=3000,          # assez haut + early stopping\n",
    "    learning_rate=0.07,       # convergence plus rapide\n",
    "    depth=8,                  # souvent optimal ici\n",
    "    l2_leaf_reg=4,            # row subsample\n",
    "    rsm=0.8,                  # feature subsample\n",
    "    loss_function=\"MultiClass\",\n",
    "    eval_metric=\"Accuracy\",\n",
    "    random_seed=42,\n",
    "    # task_type=\"GPU\",        # <--- décommente si GPU dispo\n",
    "    verbose=200\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          eval_set=(X_val, y_val),\n",
    "          cat_features=cat_features,\n",
    "          early_stopping_rounds=100)\n",
    "\n",
    "# ===== 6) Eval =====\n",
    "y_pred = model.predict(X_val).ravel().astype(int)\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "\n",
    "# pour comprendre ce qui aide\n",
    "imp = pd.Series(model.get_feature_importance(), index=X.columns).sort_values(ascending=False)\n",
    "print(\"\\nTop 12 features:\\n\", imp.head(12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "617eb6b4-ffb5-429a-97e9-c4fcbb14540d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001409 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2155\n",
      "[LightGBM] [Info] Number of data points in the train set: 12096, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score -1.945910\n",
      "[LightGBM] [Info] Start training from score -1.945910\n",
      "[LightGBM] [Info] Start training from score -1.945910\n",
      "[LightGBM] [Info] Start training from score -1.945910\n",
      "[LightGBM] [Info] Start training from score -1.945910\n",
      "[LightGBM] [Info] Start training from score -1.945910\n",
      "[LightGBM] [Info] Start training from score -1.945910\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's multi_logloss: 0.368\n",
      "Early stopping, best iteration is:\n",
      "[138]\tvalid_0's multi_logloss: 0.363318\n",
      "Accuracy: 0.8644179894179894\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "\n",
    "# --- load ---\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "y = df[\"Cover_Type\"]\n",
    "X = df.drop(columns=[\"Cover_Type\", \"Id\"])\n",
    "\n",
    "# split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# --- model ---\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=64,\n",
    "    max_depth=-1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# --- fit avec callbacks ---\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=\"multi_logloss\",\n",
    "    callbacks=[\n",
    "        early_stopping(stopping_rounds=100),\n",
    "        log_evaluation(200)   # affiche log tous les 200 itérations\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- eval ---\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e5df05c-011f-4c9f-8769-a8e7bd44f905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's multi_logloss: 0.382076\n",
      "[400]\tvalid_0's multi_logloss: 0.340878\n",
      "[600]\tvalid_0's multi_logloss: 0.339318\n",
      "Early stopping, best iteration is:\n",
      "[525]\tvalid_0's multi_logloss: 0.338593\n",
      "Accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "\n",
    "# --- load ---\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "y = df[\"Cover_Type\"].astype(int)\n",
    "X = df.drop(columns=[\"Cover_Type\", \"Id\"]).copy()\n",
    "\n",
    "# --- features qui payent ---\n",
    "X[\"Aspect_sin\"] = np.sin(np.deg2rad(df[\"Aspect\"]))\n",
    "X[\"Aspect_cos\"] = np.cos(np.deg2rad(df[\"Aspect\"]))\n",
    "X[\"Hydro_Dist\"] = np.hypot(df[\"Horizontal_Distance_To_Hydrology\"],\n",
    "                           df[\"Vertical_Distance_To_Hydrology\"])\n",
    "X[\"Near_RoadOrFire\"] = np.minimum(df[\"Horizontal_Distance_To_Roadways\"],\n",
    "                                  df[\"Horizontal_Distance_To_Fire_Points\"])\n",
    "X[\"Elev_minus_VertHydro\"] = df[\"Elevation\"] - df[\"Vertical_Distance_To_Hydrology\"]\n",
    "X[\"Road_Fire\"] = df[\"Horizontal_Distance_To_Roadways\"] + df[\"Horizontal_Distance_To_Fire_Points\"]\n",
    "X[\"Hydro_Road\"] = df[\"Horizontal_Distance_To_Hydrology\"] + df[\"Horizontal_Distance_To_Roadways\"]\n",
    "X[\"Hydro_Fire\"] = df[\"Horizontal_Distance_To_Hydrology\"] + df[\"Horizontal_Distance_To_Fire_Points\"]\n",
    "X[\"Hillshade_mean\"] = df[[\"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\"]].mean(axis=1)\n",
    "X[\"Hillshade_range\"] = df[[\"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\"]].max(axis=1) - \\\n",
    "                       df[[\"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\"]].min(axis=1)\n",
    "\n",
    "\n",
    "# --- split ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# --- modèle lightGBM bien réglé mais simple ---\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=8000,      # + d’arbres, early stopping décidera\n",
    "    learning_rate=0.02,     # un peu plus fin\n",
    "    num_leaves=384,         # plus expressif\n",
    "    min_child_samples=20,   # limite l’overfit\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_alpha=1.0,\n",
    "    reg_lambda=3.0,\n",
    "    objective=\"multiclass\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "\n",
    "fit_kwargs = dict(\n",
    "    X=X_train, y=y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=\"multi_logloss\",\n",
    "    callbacks=[early_stopping(stopping_rounds=200), log_evaluation(200)]\n",
    ")\n",
    "\n",
    "# si tu as des colonnes catégorielles uniques (pas en one-hot), passe-les ici\n",
    "if categorical_feature:\n",
    "    fit_kwargs[\"categorical_feature\"] = categorical_feature\n",
    "\n",
    "model.fit(**fit_kwargs)\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6618f943-66b6-4827-92cb-e0d2b445dcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[152]\tvalid_0's multi_logloss: 0.324772\n",
      "Accuracy: 0.8852513227513228\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from lightgbm import LGBMClassifier, early_stopping\n",
    "\n",
    "# --- load ---\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "y = df[\"Cover_Type\"].astype(int)\n",
    "X = df.drop(columns=[\"Cover_Type\", \"Id\"]).copy()\n",
    "\n",
    "# --- features utiles ---\n",
    "X[\"Aspect_sin\"] = np.sin(np.deg2rad(X[\"Aspect\"]))\n",
    "X[\"Aspect_cos\"] = np.cos(np.deg2rad(X[\"Aspect\"]))\n",
    "X[\"Hydro_Dist\"] = np.hypot(X[\"Horizontal_Distance_To_Hydrology\"],\n",
    "                           X[\"Vertical_Distance_To_Hydrology\"])\n",
    "X[\"Near_RoadOrFire\"] = np.minimum(X[\"Horizontal_Distance_To_Roadways\"],\n",
    "                                  X[\"Horizontal_Distance_To_Fire_Points\"])\n",
    "X[\"Elev_minus_VertHydro\"] = X[\"Elevation\"] - X[\"Vertical_Distance_To_Hydrology\"]\n",
    "X[\"Road_Fire\"] = X[\"Horizontal_Distance_To_Roadways\"] + X[\"Horizontal_Distance_To_Fire_Points\"]\n",
    "X[\"Hydro_Road\"] = X[\"Horizontal_Distance_To_Hydrology\"] + X[\"Horizontal_Distance_To_Roadways\"]\n",
    "X[\"Hydro_Fire\"] = X[\"Horizontal_Distance_To_Hydrology\"] + X[\"Horizontal_Distance_To_Fire_Points\"]\n",
    "X[\"Hillshade_mean\"] = X[[\"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\"]].mean(axis=1)\n",
    "X[\"Hillshade_range\"] = X[[\"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\"]].max(axis=1) - \\\n",
    "                       X[[\"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\"]].min(axis=1)\n",
    "\n",
    "# --- split ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# --- modèle ---\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=4000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=256,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"multiclass\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=\"multi_logloss\",\n",
    "    callbacks=[early_stopping(200)]\n",
    ")\n",
    "\n",
    "# --- éval ---\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1236b2e1-f688-4945-9511-e009501fddbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission.csv saved. Shape: (581012, 2)\n",
      "   Id  Cover_Type\n",
      "0   1           5\n",
      "1   2           5\n",
      "2   3           2\n",
      "3   4           2\n",
      "4   5           5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# --- load ---\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test  = pd.read_csv(\"test-full.csv\")\n",
    "\n",
    "# y / X\n",
    "y_train = train[\"Cover_Type\"].astype(int)\n",
    "X_train = train.drop(columns=[\"Cover_Type\", \"Id\"])\n",
    "X_test  = test.drop(columns=[\"Id\"])\n",
    "\n",
    "# --- model (simple & efficace) ---\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=64,\n",
    "    max_depth=-1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "# --- fit sur tout le train ---\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# --- predict sur test-full ---\n",
    "test_pred = model.predict(X_test).astype(int)\n",
    "\n",
    "# --- create submission ---\n",
    "submission = pd.DataFrame({\n",
    "    \"Id\": test[\"Id\"],\n",
    "    \"Cover_Type\": test_pred\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"submission.csv saved. Shape:\", submission.shape)\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "186f769d-1701-4fc6-8a62-90b18c70853b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[196]\tvalid_0's multi_logloss: 0.352831\n",
      "Validation Accuracy: 0.8759920634920635\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from lightgbm import LGBMClassifier, early_stopping\n",
    "\n",
    "# --- load ---\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "y = df[\"Cover_Type\"]\n",
    "X = df.drop(columns=[\"Cover_Type\", \"Id\"])\n",
    "\n",
    "# --- split (train/val pour avoir une accuracy locale) ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# --- model ---\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=4000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=128,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"multiclass\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "# --- train avec early stopping ---\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=\"multi_logloss\",\n",
    "    callbacks=[early_stopping(200)]\n",
    ")\n",
    "\n",
    "# --- eval ---\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6388e09b-2b38-40e4-9bd9-69cd1e3c2154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[147]\tvalid_0's multi_logloss: 0.333633\n",
      "\n",
      "Accuracy: 0.8806216931216931\n",
      "\n",
      "Top features (RF):\n",
      "Elev_minus_VertHydro                  0.113906\n",
      "Elevation_sq                          0.105276\n",
      "Elevation                             0.100360\n",
      "Road_Fire                             0.036131\n",
      "Horizontal_Distance_To_Roadways       0.030202\n",
      "Hydro_Road                            0.030099\n",
      "Road_minus_Hydro                      0.029760\n",
      "Fire_minus_Road                       0.026608\n",
      "Hydro_Fire                            0.025150\n",
      "Aspect_cos                            0.022789\n",
      "Wilderness_Area4                      0.022560\n",
      "Horizontal_Distance_To_Fire_Points    0.022316\n",
      "Near_RoadOrFire                       0.022177\n",
      "Hydro_Dist                            0.021572\n",
      "Fire_minus_Hydro                      0.021489\n",
      "Hillshade_9am                         0.020806\n",
      "Hydro_Dist_sq                         0.020225\n",
      "Slope_times_HydroDist                 0.018628\n",
      "Horizontal_Distance_To_Hydrology      0.017102\n",
      "Abs_VertHydro                         0.017072\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier, early_stopping\n",
    "\n",
    "# -------- 1) Helpers: feature engineering --------\n",
    "def build_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = df.drop(columns=[\"Cover_Type\", \"Id\"], errors=\"ignore\").copy()\n",
    "\n",
    "    # a) Aspect (circulaire)\n",
    "    ang = np.deg2rad(X[\"Aspect\"])\n",
    "    X[\"Aspect_sin\"] = np.sin(ang)\n",
    "    X[\"Aspect_cos\"] = np.cos(ang)\n",
    "\n",
    "    # b) Distances hydrologie + combinaisons\n",
    "    X[\"Hydro_Dist\"] = np.hypot(X[\"Horizontal_Distance_To_Hydrology\"],\n",
    "                               X[\"Vertical_Distance_To_Hydrology\"])\n",
    "    X[\"Abs_VertHydro\"] = X[\"Vertical_Distance_To_Hydrology\"].abs()\n",
    "    X[\"Elev_minus_VertHydro\"] = X[\"Elevation\"] - X[\"Vertical_Distance_To_Hydrology\"]\n",
    "\n",
    "    # c) Routes / Feux\n",
    "    X[\"Near_RoadOrFire\"] = np.minimum(X[\"Horizontal_Distance_To_Roadways\"],\n",
    "                                      X[\"Horizontal_Distance_To_Fire_Points\"])\n",
    "    X[\"Road_Fire\"] = X[\"Horizontal_Distance_To_Roadways\"] + X[\"Horizontal_Distance_To_Fire_Points\"]\n",
    "    X[\"Hydro_Road\"] = X[\"Horizontal_Distance_To_Hydrology\"] + X[\"Horizontal_Distance_To_Roadways\"]\n",
    "    X[\"Hydro_Fire\"] = X[\"Horizontal_Distance_To_Hydrology\"] + X[\"Horizontal_Distance_To_Fire_Points\"]\n",
    "\n",
    "    # d) Hillshade\n",
    "    hs_cols = [\"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\"]\n",
    "    X[\"Hillshade_mean\"] = X[hs_cols].mean(axis=1)\n",
    "    X[\"Hillshade_range\"] = X[hs_cols].max(axis=1) - X[hs_cols].min(axis=1)\n",
    "    X[\"Hillshade_sum\"] = X[hs_cols].sum(axis=1)\n",
    "\n",
    "    # e) Pente & interactions\n",
    "    X[\"Slope_times_HydroDist\"] = X[\"Slope\"] * X[\"Hydro_Dist\"]\n",
    "    X[\"Slope_times_Elev\"] = X[\"Slope\"] * X[\"Elevation\"]\n",
    "    X[\"Elev_over_Slope\"] = X[\"Elevation\"] / (X[\"Slope\"] + 1e-6)\n",
    "\n",
    "    # f) Différences horizontales\n",
    "    X[\"Road_minus_Hydro\"] = X[\"Horizontal_Distance_To_Roadways\"] - X[\"Horizontal_Distance_To_Hydrology\"]\n",
    "    X[\"Fire_minus_Road\"]  = X[\"Horizontal_Distance_To_Fire_Points\"] - X[\"Horizontal_Distance_To_Roadways\"]\n",
    "    X[\"Fire_minus_Hydro\"] = X[\"Horizontal_Distance_To_Fire_Points\"] - X[\"Horizontal_Distance_To_Hydrology\"]\n",
    "\n",
    "    # g) Non-linéaires\n",
    "    for col in [\"Elevation\", \"Slope\", \"Hydro_Dist\"]:\n",
    "        X[f\"{col}_sq\"] = X[col] * X[col]\n",
    "\n",
    "    return X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# -------- 2) Load & FE --------\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "y = df[\"Cover_Type\"].astype(int)\n",
    "X = build_features(df)\n",
    "\n",
    "# -------- 3) Split --------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# -------- 4) Feature selection via RandomForest --------\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=500, random_state=42, n_jobs=-1, max_features=\"sqrt\"\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "importances = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "TOP_K = min(40, len(importances))\n",
    "top_features = importances.index[:TOP_K].tolist()\n",
    "\n",
    "# -------- 5) Train LightGBM sur les features sélectionnées --------\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=4000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=256,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"multiclass\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train[top_features], y_train,\n",
    "    eval_set=[(X_val[top_features], y_val)],\n",
    "    eval_metric=\"multi_logloss\",\n",
    "    callbacks=[early_stopping(200)]\n",
    ")\n",
    "\n",
    "# -------- 6) Eval --------\n",
    "y_pred = model.predict(X_val[top_features])\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_val, y_pred))\n",
    "\n",
    "# -------- 7) Print top features à la fin --------\n",
    "print(\"\\nTop features (RF):\")\n",
    "print(importances.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5379b50-34cc-494d-b1d6-52aa946f60db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[148]\tvalid_0's multi_logloss: 0.314672\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[161]\tvalid_0's multi_logloss: 0.338869\n",
      "Accuracy (all features):  0.887566\n",
      "Accuracy (top 20):    0.877976\n",
      "\n",
      "Top features (RF):\n",
      "Elev_minus_VertHydro                  0.113906\n",
      "Elevation_sq                          0.105276\n",
      "Elevation                             0.100360\n",
      "Road_Fire                             0.036131\n",
      "Horizontal_Distance_To_Roadways       0.030202\n",
      "Hydro_Road                            0.030099\n",
      "Road_minus_Hydro                      0.029760\n",
      "Fire_minus_Road                       0.026608\n",
      "Hydro_Fire                            0.025150\n",
      "Aspect_cos                            0.022789\n",
      "Wilderness_Area4                      0.022560\n",
      "Horizontal_Distance_To_Fire_Points    0.022316\n",
      "Near_RoadOrFire                       0.022177\n",
      "Hydro_Dist                            0.021572\n",
      "Fire_minus_Hydro                      0.021489\n",
      "Hillshade_9am                         0.020806\n",
      "Hydro_Dist_sq                         0.020225\n",
      "Slope_times_HydroDist                 0.018628\n",
      "Horizontal_Distance_To_Hydrology      0.017102\n",
      "Abs_VertHydro                         0.017072\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier, early_stopping\n",
    "\n",
    "# ---------- Feature engineering minimal ----------\n",
    "def build_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = df.drop(columns=[\"Cover_Type\", \"Id\"], errors=\"ignore\").copy()\n",
    "\n",
    "    # Aspect (circulaire)\n",
    "    ang = np.deg2rad(X[\"Aspect\"])\n",
    "    X[\"Aspect_sin\"] = np.sin(ang)\n",
    "    X[\"Aspect_cos\"] = np.cos(ang)\n",
    "\n",
    "    # Distances & combinaisons\n",
    "    X[\"Hydro_Dist\"] = np.hypot(X[\"Horizontal_Distance_To_Hydrology\"],\n",
    "                               X[\"Vertical_Distance_To_Hydrology\"])\n",
    "    X[\"Abs_VertHydro\"] = X[\"Vertical_Distance_To_Hydrology\"].abs()\n",
    "    X[\"Elev_minus_VertHydro\"] = X[\"Elevation\"] - X[\"Vertical_Distance_To_Hydrology\"]\n",
    "    X[\"Near_RoadOrFire\"] = np.minimum(X[\"Horizontal_Distance_To_Roadways\"],\n",
    "                                      X[\"Horizontal_Distance_To_Fire_Points\"])\n",
    "    X[\"Road_Fire\"]  = X[\"Horizontal_Distance_To_Roadways\"] + X[\"Horizontal_Distance_To_Fire_Points\"]\n",
    "    X[\"Hydro_Road\"] = X[\"Horizontal_Distance_To_Hydrology\"] + X[\"Horizontal_Distance_To_Roadways\"]\n",
    "    X[\"Hydro_Fire\"] = X[\"Horizontal_Distance_To_Hydrology\"] + X[\"Horizontal_Distance_To_Fire_Points\"]\n",
    "\n",
    "    # Hillshade\n",
    "    hs = [\"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\"]\n",
    "    X[\"Hillshade_mean\"]  = X[hs].mean(axis=1)\n",
    "    X[\"Hillshade_range\"] = X[hs].max(axis=1) - X[hs].min(axis=1)\n",
    "    X[\"Hillshade_sum\"]   = X[hs].sum(axis=1)\n",
    "\n",
    "    # Interactions simples + non-linéaires\n",
    "    X[\"Slope_times_HydroDist\"] = X[\"Slope\"] * X[\"Hydro_Dist\"]\n",
    "    X[\"Slope_times_Elev\"]      = X[\"Slope\"] * X[\"Elevation\"]\n",
    "    X[\"Elev_over_Slope\"]       = X[\"Elevation\"] / (X[\"Slope\"] + 1e-6)\n",
    "    X[\"Road_minus_Hydro\"]      = X[\"Horizontal_Distance_To_Roadways\"] - X[\"Horizontal_Distance_To_Hydrology\"]\n",
    "    X[\"Fire_minus_Road\"]       = X[\"Horizontal_Distance_To_Fire_Points\"] - X[\"Horizontal_Distance_To_Roadways\"]\n",
    "    X[\"Fire_minus_Hydro\"]      = X[\"Horizontal_Distance_To_Fire_Points\"] - X[\"Horizontal_Distance_To_Hydrology\"]\n",
    "\n",
    "    for col in [\"Elevation\", \"Slope\", \"Hydro_Dist\"]:\n",
    "        X[f\"{col}_sq\"] = X[col] * X[col]\n",
    "\n",
    "    return X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# ---------- Load + FE ----------\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "y = df[\"Cover_Type\"].astype(int)\n",
    "X = build_features(df)\n",
    "\n",
    "# ---------- Split ----------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ---------- Baseline LGBM sur TOUTES les features ----------\n",
    "model_all = LGBMClassifier(\n",
    "    n_estimators=4000, learning_rate=0.03, num_leaves=256,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    objective=\"multiclass\", random_state=42, n_jobs=-1, verbosity=-1\n",
    ")\n",
    "model_all.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=\"multi_logloss\",\n",
    "    callbacks=[early_stopping(200)]\n",
    ")\n",
    "pred_all = model_all.predict(X_val)\n",
    "acc_all = accuracy_score(y_val, pred_all)\n",
    "\n",
    "# ---------- Sélection top-20 via RandomForest ----------\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=500, random_state=42, n_jobs=-1, max_features=\"sqrt\"\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "importances = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "TOP_K = min(20, len(importances))\n",
    "top_features = importances.index[:TOP_K].tolist()\n",
    "\n",
    "# ---------- LGBM sur Top-20 ----------\n",
    "model_top = LGBMClassifier(\n",
    "    n_estimators=4000, learning_rate=0.03, num_leaves=256,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    objective=\"multiclass\", random_state=7, n_jobs=-1, verbosity=-1\n",
    ")\n",
    "model_top.fit(\n",
    "    X_train[top_features], y_train,\n",
    "    eval_set=[(X_val[top_features], y_val)],\n",
    "    eval_metric=\"multi_logloss\",\n",
    "    callbacks=[early_stopping(200)]\n",
    ")\n",
    "pred_top = model_top.predict(X_val[top_features])\n",
    "acc_top = accuracy_score(y_val, pred_top)\n",
    "\n",
    "# ---------- Résultats ----------\n",
    "print(f\"Accuracy (all features):  {acc_all:.6f}\")\n",
    "print(f\"Accuracy (top {TOP_K}):    {acc_top:.6f}\")\n",
    "print(\"\\nTop features (RF):\")\n",
    "print(importances.head(TOP_K))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa8d314-874b-4c60-aa45-d612f222c2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from lightgbm import LGBMClassifier, early_stopping\n",
    "\n",
    "# --- load ---\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test  = pd.read_csv(\"test-full.csv\")\n",
    "\n",
    "y = train[\"Cover_Type\"].astype(int)\n",
    "X = train.drop(columns=[\"Cover_Type\", \"Id\"])\n",
    "X_test = test.drop(columns=[\"Id\"])\n",
    "\n",
    "# sécurité: même colonnes entre train et test\n",
    "assert list(X.columns) == list(X_test.columns), \"Mismatch colonnes train/test\"\n",
    "\n",
    "# --- split pour une accuracy locale ---\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# --- modèle ---\n",
    "base_params = dict(\n",
    "    n_estimators=4000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=128,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"multiclass\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=-1\n",
    ")\n",
    "model = LGBMClassifier(**base_params)\n",
    "\n",
    "# --- fit avec early stopping (sur le split) ---\n",
    "model.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=\"multi_logloss\",\n",
    "    callbacks=[early_stopping(200)]\n",
    ")\n",
    "\n",
    "# --- accuracy locale ---\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "\n",
    "# --- re-fit sur TOUT le train avec le meilleur nombre d'itérations ---\n",
    "best_iters = getattr(model, \"best_iteration_\", None)\n",
    "final_model = LGBMClassifier(**base_params)\n",
    "if best_iters is not None:\n",
    "    final_model.set_params(n_estimators=best_iters)\n",
    "\n",
    "final_model.fit(X, y)  # pas d'eval_set ici: on utilise tout le train\n",
    "\n",
    "# --- prédiction sur test-full + sauvegarde submission ---\n",
    "test_pred = final_model.predict(X_test).astype(int)\n",
    "submission = pd.DataFrame({\"Id\": test[\"Id\"], \"Cover_Type\": test_pred})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"submission.csv saved:\", submission.shape)\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d76c90bc-e81a-4a2a-a453-7ab89e7a6912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[196]\tvalid_0's multi_logloss: 0.352831\n",
      "Validation Accuracy: 0.8759920634920635\n",
      "Best iterations: 196\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from lightgbm import LGBMClassifier, early_stopping\n",
    "\n",
    "# load\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "y = df[\"Cover_Type\"].astype(int)\n",
    "X = df.drop(columns=[\"Cover_Type\", \"Id\"])\n",
    "\n",
    "# split\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# model\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=4000, learning_rate=0.03, num_leaves=128,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    objective=\"multiclass\", random_state=42, n_jobs=-1, verbosity=-1\n",
    ")\n",
    "\n",
    "# train + early stopping\n",
    "model.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=\"multi_logloss\",\n",
    "    callbacks=[early_stopping(200)]\n",
    ")\n",
    "\n",
    "# accuracy locale\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "\n",
    "# garder le meilleur nb d'itérations pour la phase finale\n",
    "best_iters = getattr(model, \"best_iteration_\", None)\n",
    "print(\"Best iterations:\", best_iters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9587fc54-515a-4158-9569-9b9836502d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission.csv saved: (581012, 2)\n",
      "   Id  Cover_Type\n",
      "0   1           5\n",
      "1   2           5\n",
      "2   3           2\n",
      "3   4           2\n",
      "4   5           5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='submission.csv' target='_blank'>submission.csv</a><br>"
      ],
      "text/plain": [
       "C:\\Users\\andre\\x hec\\kaggle challeng\\submission.csv"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# reload data (propre)\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test  = pd.read_csv(\"test-full.csv\")\n",
    "\n",
    "y = train[\"Cover_Type\"].astype(int)\n",
    "X = train.drop(columns=[\"Cover_Type\", \"Id\"])\n",
    "X_test = test.drop(columns=[\"Id\"])\n",
    "\n",
    "# même colonnes train/test\n",
    "assert list(X.columns) == list(X_test.columns)\n",
    "\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=(196),  # utilise la meilleure itération si dispo\n",
    "    learning_rate=0.03, num_leaves=128,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    objective=\"multiclass\", random_state=42, n_jobs=-1, verbosity=-1\n",
    ")\n",
    "\n",
    "# fit sur 100% du train (pas d'early stopping ici)\n",
    "model.fit(X, y)\n",
    "\n",
    "# prédire test + sauver submission\n",
    "pred = model.predict(X_test).astype(int)\n",
    "submission = pd.DataFrame({\"Id\": test[\"Id\"], \"Cover_Type\": pred})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"submission.csv saved:\", submission.shape)\n",
    "print(submission.head())\n",
    "from IPython.display import FileLink\n",
    "FileLink(\"submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68f043f0-b65b-4275-9e58-b322c2ff895e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[612]\tvalid_0's multi_logloss: 0.323917\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[559]\tvalid_0's multi_logloss: 0.319499\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[675]\tvalid_0's multi_logloss: 0.315029\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[574]\tvalid_0's multi_logloss: 0.311972\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[481]\tvalid_0's multi_logloss: 0.376001\n",
      "CV OOF Accuracy: 0.880291 | mean best_iter: 580\n",
      "submission.csv saved: (581012, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from lightgbm import LGBMClassifier, early_stopping\n",
    "\n",
    "# ---------- 1) Feature engineering unique (train & test) ----------\n",
    "def build_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = df.drop(columns=[\"Cover_Type\", \"Id\"], errors=\"ignore\").copy()\n",
    "    ang = np.deg2rad(X[\"Aspect\"])\n",
    "    X[\"Aspect_sin\"] = np.sin(ang); X[\"Aspect_cos\"] = np.cos(ang)\n",
    "    X[\"Hydro_Dist\"] = np.hypot(X[\"Horizontal_Distance_To_Hydrology\"],\n",
    "                               X[\"Vertical_Distance_To_Hydrology\"])\n",
    "    X[\"Near_RoadOrFire\"] = np.minimum(X[\"Horizontal_Distance_To_Roadways\"],\n",
    "                                      X[\"Horizontal_Distance_To_Fire_Points\"])\n",
    "    X[\"Elev_minus_VertHydro\"] = X[\"Elevation\"] - X[\"Vertical_Distance_To_Hydrology\"]\n",
    "    X[\"Road_Fire\"]  = X[\"Horizontal_Distance_To_Roadways\"] + X[\"Horizontal_Distance_To_Fire_Points\"]\n",
    "    X[\"Hydro_Road\"] = X[\"Horizontal_Distance_To_Hydrology\"] + X[\"Horizontal_Distance_To_Roadways\"]\n",
    "    X[\"Hydro_Fire\"] = X[\"Horizontal_Distance_To_Hydrology\"] + X[\"Horizontal_Distance_To_Fire_Points\"]\n",
    "    hs = [\"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\"]\n",
    "    X[\"Hillshade_mean\"]  = X[hs].mean(axis=1)\n",
    "    X[\"Hillshade_range\"] = X[hs].max(axis=1) - X[hs].min(axis=1)\n",
    "    # petits bonus souvent payants\n",
    "    X[\"Abs_VertHydro\"] = X[\"Vertical_Distance_To_Hydrology\"].abs()\n",
    "    X[\"Slope_times_HydroDist\"] = X[\"Slope\"] * X[\"Hydro_Dist\"]\n",
    "    X[\"Elev_over_Slope\"] = X[\"Elevation\"] / (X[\"Slope\"] + 1e-6)\n",
    "    for c in [\"Elevation\",\"Slope\",\"Hydro_Dist\"]:\n",
    "        X[f\"{c}_sq\"] = X[c]*X[c]\n",
    "    return X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# ---------- 2) Data ----------\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test  = pd.read_csv(\"test-full.csv\")\n",
    "\n",
    "y = train[\"Cover_Type\"].astype(int).values\n",
    "X = build_features(train)\n",
    "X_test = build_features(test)\n",
    "assert list(X.columns) == list(X_test.columns), \"Colonnes train/test différentes\"\n",
    "\n",
    "classes_ = np.sort(np.unique(y))\n",
    "n_classes = len(classes_)\n",
    "\n",
    "# ---------- 3) 5-fold CV + early stopping + moyenne ----------\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "oof = np.zeros((len(X), n_classes))\n",
    "test_pred = np.zeros((len(X_test), n_classes))\n",
    "best_iters = []\n",
    "\n",
    "params = dict(\n",
    "    n_estimators=8000, learning_rate=0.02, num_leaves=256,\n",
    "    min_child_samples=20, subsample=0.9, colsample_bytree=0.9,\n",
    "    reg_alpha=1.0, reg_lambda=2.0,\n",
    "    objective=\"multiclass\", random_state=42, n_jobs=-1, verbosity=-1\n",
    ")\n",
    "\n",
    "for fold, (tr, va) in enumerate(skf.split(X, y), 1):\n",
    "    model = LGBMClassifier(**{**params, \"random_state\": 42+fold})\n",
    "    model.fit(\n",
    "        X.iloc[tr], y[tr],\n",
    "        eval_set=[(X.iloc[va], y[va])],\n",
    "        eval_metric=\"multi_logloss\",\n",
    "        callbacks=[early_stopping(300)]\n",
    "    )\n",
    "    oof[va] = model.predict_proba(X.iloc[va])\n",
    "    test_pred += model.predict_proba(X_test) / skf.n_splits\n",
    "    best_iters.append(getattr(model, \"best_iteration_\", params[\"n_estimators\"]))\n",
    "\n",
    "# CV accuracy (plus fiable qu’un seul split)\n",
    "oof_labels = classes_[oof.argmax(axis=1)]\n",
    "cv_acc = accuracy_score(y, oof_labels)\n",
    "print(f\"CV OOF Accuracy: {cv_acc:.6f} | mean best_iter: {np.mean(best_iters):.0f}\")\n",
    "\n",
    "# ---------- 4) Réentraînement sur 100% du train au bon nb d’itérations ----------\n",
    "final_n_estimators = int(np.mean(best_iters))\n",
    "final = LGBMClassifier(**{**params, \"n_estimators\": final_n_estimators, \"random_state\": 42})\n",
    "final.fit(X, y)\n",
    "\n",
    "# ---------- 5) Submission ----------\n",
    "test_labels = classes_[test_pred.argmax(axis=1)]  # moyenne des folds\n",
    "submission = pd.DataFrame({\"Id\": test[\"Id\"], \"Cover_Type\": test_labels})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"submission.csv saved:\", submission.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fcb5ea83-a794-4b3f-baaf-1ce6d50141da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15120, 28), (581012, 28))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test  = pd.read_csv(\"test-full.csv\")\n",
    "\n",
    "y = train[\"Cover_Type\"].astype(int)\n",
    "X = train.drop(columns=[\"Cover_Type\", \"Id\"])\n",
    "X_test = test.drop(columns=[\"Id\"])\n",
    "\n",
    "assert list(X.columns) == list(X_test.columns)\n",
    "\n",
    "def make_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # Aspect wraps around (0≈360): trig works much better than raw degrees\n",
    "    asp = np.deg2rad(out[\"Aspect\"].astype(np.float32))\n",
    "    out[\"Aspect_sin\"] = np.sin(asp)\n",
    "    out[\"Aspect_cos\"] = np.cos(asp)\n",
    "\n",
    "    # Hydro geometry\n",
    "    h_hyd = out[\"Horizontal_Distance_To_Hydrology\"].astype(np.float32)\n",
    "    v_hyd = out[\"Vertical_Distance_To_Hydrology\"].astype(np.float32)\n",
    "    out[\"Hydro_Euclid\"] = np.sqrt(h_hyd**2 + v_hyd**2)\n",
    "\n",
    "    # Pairwise distance combos (very predictive)\n",
    "    h_road = out[\"Horizontal_Distance_To_Roadways\"].astype(np.float32)\n",
    "    h_fire = out[\"Horizontal_Distance_To_Fire_Points\"].astype(np.float32)\n",
    "    out[\"Road_Fire_Diff\"]  = (h_road - h_fire).abs()\n",
    "    out[\"Road_Hydro_Diff\"] = (h_road - h_hyd).abs()\n",
    "    out[\"Fire_Hydro_Diff\"] = (h_fire - h_hyd).abs()\n",
    "    out[\"Road_Fire_Sum\"]   = h_road + h_fire\n",
    "\n",
    "    # Elevation interactions with hydrology\n",
    "    elev = out[\"Elevation\"].astype(np.float32)\n",
    "    out[\"Elev_minus_VertHydro\"] = elev - v_hyd\n",
    "    out[\"Elev_plus_VertHydro\"]  = elev + v_hyd\n",
    "    out[\"VertHydro_over_Elev\"]  = v_hyd / (elev + 1e-3)\n",
    "\n",
    "    # Hillshade summaries/differences\n",
    "    hs9  = out[\"Hillshade_9am\"].astype(np.float32)\n",
    "    hsn  = out[\"Hillshade_Noon\"].astype(np.float32)\n",
    "    hs3  = out[\"Hillshade_3pm\"].astype(np.float32)\n",
    "    out[\"Hillshade_Mean\"]  = (hs9 + hsn + hs3) / 3.0\n",
    "    out[\"Hillshade_Range\"] = pd.concat([hs9, hsn, hs3], axis=1).max(axis=1) - \\\n",
    "                             pd.concat([hs9, hsn, hs3], axis=1).min(axis=1)\n",
    "    out[\"Hillshade_Noon_minus_9am\"] = hsn - hs9\n",
    "    out[\"Hillshade_3pm_minus_Noon\"] = hs3 - hsn\n",
    "\n",
    "    # Simple slope flags\n",
    "    out[\"Is_Flat\"]  = (out[\"Slope\"] == 0).astype(np.int8)\n",
    "    out[\"Is_Steep\"] = (out[\"Slope\"] >= 25).astype(np.int8)\n",
    "\n",
    "    # Collapse one-hot Wilderness/Soil into single integer categories\n",
    "    soil_cols = [c for c in out.columns if c.startswith(\"Soil_Type\")]\n",
    "    wild_cols = [c for c in out.columns if c.startswith(\"Wilderness_Area\")]\n",
    "\n",
    "    out[\"Soil\"] = (out[soil_cols].values.argmax(axis=1) + 1).astype(np.int16)\n",
    "    out[\"Wilderness\"] = (out[wild_cols].values.argmax(axis=1) + 1).astype(np.int8)\n",
    "\n",
    "    # Drop original one-hots (we’ll treat Soil/Wilderness as categorical)\n",
    "    out = out.drop(columns=soil_cols + wild_cols)\n",
    "\n",
    "    return out\n",
    "\n",
    "X_fe      = make_features(X)\n",
    "X_test_fe = make_features(X_test)\n",
    "X_fe.shape, X_test_fe.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48deb2ba-a734-4f16-9a6a-e6e30e8a8047",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LGBMClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m y_tr, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\u001b[32m     36\u001b[39m model = LGBMClassifier(**lgb_params)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmulti_logloss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcat_feats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\n\u001b[32m     44\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m val_proba = model.predict_proba(X_val, num_iteration=model.best_iteration_)\n\u001b[32m     47\u001b[39m oof_proba[val_idx] = val_proba\n",
      "\u001b[31mTypeError\u001b[39m: LGBMClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from lightgbm import LGBMClassifier\n",
    "import numpy as np\n",
    "\n",
    "N_FOLDS = 5\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "lgb_params = dict(\n",
    "    objective=\"multiclass\",\n",
    "    learning_rate=0.03,\n",
    "    n_estimators=10000,           # big cap; early stopping will pick ~200\n",
    "    num_leaves=255,               # a bit larger than 128 usually helps here\n",
    "    max_depth=-1,\n",
    "    min_child_samples=60,         # regularization\n",
    "    subsample=0.8,\n",
    "    subsample_freq=1,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.2,\n",
    "    reg_lambda=0.6,\n",
    "    extra_trees=True,             # stabilizes and helps generalization\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "oof_proba  = np.zeros((len(X_fe), 7), dtype=np.float32)\n",
    "test_proba = np.zeros((len(X_test_fe), 7), dtype=np.float32)\n",
    "fold_scores = []\n",
    "cat_feats = [\"Soil\", \"Wilderness\"]  # categorical columns in X_fe\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(X_fe, y), 1):\n",
    "    X_tr, X_val = X_fe.iloc[trn_idx], X_fe.iloc[val_idx]\n",
    "    y_tr, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n",
    "\n",
    "    model = LGBMClassifier(**lgb_params)\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric=\"multi_logloss\",\n",
    "        categorical_feature=cat_feats,\n",
    "        early_stopping_rounds=200,\n",
    "        verbose=200\n",
    "    )\n",
    "\n",
    "    val_proba = model.predict_proba(X_val, num_iteration=model.best_iteration_)\n",
    "    oof_proba[val_idx] = val_proba\n",
    "    val_pred = model.classes_[val_proba.argmax(axis=1)]\n",
    "    acc = accuracy_score(y_val, val_pred)\n",
    "    fold_scores.append(acc)\n",
    "    print(f\"Fold {fold} | acc={acc:.5f} | best_iter={model.best_iteration_}\")\n",
    "\n",
    "    test_proba += model.predict_proba(X_test_fe, num_iteration=model.best_iteration_) / N_FOLDS\n",
    "\n",
    "oof_pred = model.classes_[oof_proba.argmax(axis=1)]\n",
    "cv_acc = accuracy_score(y, oof_pred)\n",
    "print(f\"\\nCV mean accuracy: {np.mean(fold_scores):.5f} ± {np.std(fold_scores):.5f}\")\n",
    "print(f\"OOF accuracy    : {cv_acc:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f0a5582-88e7-4125-8de1-739da3c94da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2491]\tvalid_0's multi_logloss: 0.337494\n",
      "Fold 1 | acc=0.87269 | best_iter=2491\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3022]\tvalid_0's multi_logloss: 0.315401\n",
      "Fold 2 | acc=0.89153 | best_iter=3022\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2024]\tvalid_0's multi_logloss: 0.331558\n",
      "Fold 3 | acc=0.87235 | best_iter=2024\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3131]\tvalid_0's multi_logloss: 0.324175\n",
      "Fold 4 | acc=0.88029 | best_iter=3131\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2405]\tvalid_0's multi_logloss: 0.373379\n",
      "Fold 5 | acc=0.86640 | best_iter=2405\n",
      "\n",
      "Moyenne CV accuracy: 0.8766534391534391\n",
      "OOF accuracy        : 0.8766534391534392\n",
      "\n",
      "✅ submission.csv créé avec shape (581012, 2)\n",
      "   Id  Cover_Type\n",
      "0   1           5\n",
      "1   2           5\n",
      "2   3           2\n",
      "3   4           2\n",
      "4   5           5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from lightgbm import LGBMClassifier, early_stopping\n",
    "\n",
    "# ======================\n",
    "# 1) Chargement des données\n",
    "# ======================\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test  = pd.read_csv(\"test-full.csv\")\n",
    "\n",
    "y = train[\"Cover_Type\"].astype(int)\n",
    "X = train.drop(columns=[\"Cover_Type\", \"Id\"])\n",
    "X_test = test.drop(columns=[\"Id\"])\n",
    "\n",
    "assert list(X.columns) == list(X_test.columns)\n",
    "\n",
    "# ======================\n",
    "# 2) Feature engineering\n",
    "# ======================\n",
    "def make_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # Aspect trigonométrique\n",
    "    asp = np.deg2rad(out[\"Aspect\"].astype(np.float32))\n",
    "    out[\"Aspect_sin\"] = np.sin(asp)\n",
    "    out[\"Aspect_cos\"] = np.cos(asp)\n",
    "\n",
    "    # Distances hydrologie\n",
    "    h_hyd = out[\"Horizontal_Distance_To_Hydrology\"].astype(np.float32)\n",
    "    v_hyd = out[\"Vertical_Distance_To_Hydrology\"].astype(np.float32)\n",
    "    out[\"Hydro_Euclid\"] = np.sqrt(h_hyd**2 + v_hyd**2)\n",
    "\n",
    "    # Interactions distances\n",
    "    h_road = out[\"Horizontal_Distance_To_Roadways\"].astype(np.float32)\n",
    "    h_fire = out[\"Horizontal_Distance_To_Fire_Points\"].astype(np.float32)\n",
    "    out[\"Road_Fire_Diff\"]  = (h_road - h_fire).abs()\n",
    "    out[\"Road_Hydro_Diff\"] = (h_road - h_hyd).abs()\n",
    "    out[\"Fire_Hydro_Diff\"] = (h_fire - h_hyd).abs()\n",
    "    out[\"Road_Fire_Sum\"]   = h_road + h_fire\n",
    "\n",
    "    # Liens avec l’élévation\n",
    "    elev = out[\"Elevation\"].astype(np.float32)\n",
    "    out[\"Elev_minus_VertHydro\"] = elev - v_hyd\n",
    "    out[\"Elev_plus_VertHydro\"]  = elev + v_hyd\n",
    "    out[\"VertHydro_over_Elev\"]  = v_hyd / (elev + 1e-3)\n",
    "\n",
    "    # Hillshade\n",
    "    hs9, hsn, hs3 = out[\"Hillshade_9am\"], out[\"Hillshade_Noon\"], out[\"Hillshade_3pm\"]\n",
    "    out[\"Hillshade_Mean\"]  = (hs9 + hsn + hs3) / 3.0\n",
    "    out[\"Hillshade_Range\"] = pd.concat([hs9, hsn, hs3], axis=1).max(axis=1) - \\\n",
    "                             pd.concat([hs9, hsn, hs3], axis=1).min(axis=1)\n",
    "    out[\"Hillshade_Noon_minus_9am\"] = hsn - hs9\n",
    "    out[\"Hillshade_3pm_minus_Noon\"] = hs3 - hsn\n",
    "\n",
    "    # Flags pente\n",
    "    out[\"Is_Flat\"]  = (out[\"Slope\"] == 0).astype(np.int8)\n",
    "    out[\"Is_Steep\"] = (out[\"Slope\"] >= 25).astype(np.int8)\n",
    "\n",
    "    # Catégories soil & wilderness\n",
    "    soil_cols = [c for c in out.columns if c.startswith(\"Soil_Type\")]\n",
    "    wild_cols = [c for c in out.columns if c.startswith(\"Wilderness_Area\")]\n",
    "\n",
    "    out[\"Soil\"] = (out[soil_cols].values.argmax(axis=1) + 1).astype(np.int16)\n",
    "    out[\"Wilderness\"] = (out[wild_cols].values.argmax(axis=1) + 1).astype(np.int8)\n",
    "\n",
    "    # Supprimer one-hot\n",
    "    out = out.drop(columns=soil_cols + wild_cols)\n",
    "    return out\n",
    "\n",
    "X_fe      = make_features(X)\n",
    "X_test_fe = make_features(X_test)\n",
    "\n",
    "# ======================\n",
    "# 3) Modèle LightGBM avec CV\n",
    "# ======================\n",
    "params = dict(\n",
    "    objective=\"multiclass\",\n",
    "    learning_rate=0.03,\n",
    "    n_estimators=10000,\n",
    "    num_leaves=255,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=60,\n",
    "    subsample=0.8,\n",
    "    subsample_freq=1,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.2,\n",
    "    reg_lambda=0.6,\n",
    "    extra_trees=True,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "N_FOLDS = 5\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "oof_proba  = np.zeros((len(X_fe), 7), dtype=np.float32)\n",
    "test_proba = np.zeros((len(X_test_fe), 7), dtype=np.float32)\n",
    "scores = []\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(X_fe, y), 1):\n",
    "    X_tr, X_val = X_fe.iloc[trn_idx], X_fe.iloc[val_idx]\n",
    "    y_tr, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n",
    "\n",
    "    model = LGBMClassifier(**params)\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric=\"multi_logloss\",\n",
    "        categorical_feature=[\"Soil\",\"Wilderness\"],\n",
    "        callbacks=[early_stopping(200, verbose=True)]\n",
    "    )\n",
    "\n",
    "    # Validation\n",
    "    val_proba = model.predict_proba(X_val, num_iteration=model.best_iteration_)\n",
    "    oof_proba[val_idx] = val_proba\n",
    "    val_pred = model.classes_[val_proba.argmax(axis=1)]\n",
    "    acc = accuracy_score(y_val, val_pred)\n",
    "    scores.append(acc)\n",
    "    print(f\"Fold {fold} | acc={acc:.5f} | best_iter={model.best_iteration_}\")\n",
    "\n",
    "    # Moyenne des prédictions test\n",
    "    test_proba += model.predict_proba(X_test_fe, num_iteration=model.best_iteration_) / N_FOLDS\n",
    "\n",
    "# ======================\n",
    "# 4) Accuracy finale + Submission\n",
    "# ======================\n",
    "oof_pred = model.classes_[oof_proba.argmax(axis=1)]\n",
    "cv_acc = accuracy_score(y, oof_pred)\n",
    "print(\"\\nMoyenne CV accuracy:\", np.mean(scores))\n",
    "print(\"OOF accuracy        :\", cv_acc)\n",
    "\n",
    "final_pred = model.classes_[test_proba.argmax(axis=1)]\n",
    "submission = pd.DataFrame({\"Id\": test[\"Id\"], \"Cover_Type\": final_pred.astype(int)})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\n✅ submission.csv créé avec shape\", submission.shape)\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31744e0d-033e-4f3a-842c-782927782e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[150]\tvalid_0's multi_logloss: 0.329207\n",
      "\n",
      "Validation Accuracy: 0.88492\n",
      "Best iteration retenue: 150\n",
      "\n",
      "✅ Fichier CSV créé: submission.csv  | shape=(581012, 2)\n",
      "✅ Archive ZIP créée: submission.csv.zip\n",
      "   Id  Cover_Type\n",
      "0   1           5\n",
      "1   2           5\n",
      "2   3           2\n",
      "3   4           2\n",
      "4   5           5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from lightgbm import LGBMClassifier, early_stopping\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# ============== Config ==============\n",
    "SEED = 42\n",
    "VAL_SIZE = 0.20\n",
    "EARLY_STOP_ROUNDS = 200\n",
    "\n",
    "# ============== Helpers ==============\n",
    "def make_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = df.copy()\n",
    "\n",
    "    # --- features utiles (ta version \"best\") ---\n",
    "    X[\"Aspect_sin\"] = np.sin(np.deg2rad(X[\"Aspect\"]))\n",
    "    X[\"Aspect_cos\"] = np.cos(np.deg2rad(X[\"Aspect\"]))\n",
    "\n",
    "    X[\"Hydro_Dist\"] = np.hypot(\n",
    "        X[\"Horizontal_Distance_To_Hydrology\"],\n",
    "        X[\"Vertical_Distance_To_Hydrology\"]\n",
    "    )\n",
    "\n",
    "    X[\"Near_RoadOrFire\"] = np.minimum(\n",
    "        X[\"Horizontal_Distance_To_Roadways\"],\n",
    "        X[\"Horizontal_Distance_To_Fire_Points\"]\n",
    "    )\n",
    "\n",
    "    X[\"Elev_minus_VertHydro\"] = X[\"Elevation\"] - X[\"Vertical_Distance_To_Hydrology\"]\n",
    "\n",
    "    X[\"Road_Fire\"] = (\n",
    "        X[\"Horizontal_Distance_To_Roadways\"] + X[\"Horizontal_Distance_To_Fire_Points\"]\n",
    "    )\n",
    "    X[\"Hydro_Road\"] = (\n",
    "        X[\"Horizontal_Distance_To_Hydrology\"] + X[\"Horizontal_Distance_To_Roadways\"]\n",
    "    )\n",
    "    X[\"Hydro_Fire\"] = (\n",
    "        X[\"Horizontal_Distance_To_Hydrology\"] + X[\"Horizontal_Distance_To_Fire_Points\"]\n",
    "    )\n",
    "\n",
    "    X[\"Hillshade_mean\"] = X[[\"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\"]].mean(axis=1)\n",
    "    X[\"Hillshade_range\"] = (\n",
    "        X[[\"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\"]].max(axis=1) -\n",
    "        X[[\"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\"]].min(axis=1)\n",
    "    )\n",
    "\n",
    "    return X\n",
    "\n",
    "# ============== 1) Load ==============\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test  = pd.read_csv(\"test-full.csv\")\n",
    "\n",
    "y = train[\"Cover_Type\"].astype(int)\n",
    "X_base = train.drop(columns=[\"Cover_Type\", \"Id\"]).copy()\n",
    "X_test_base = test.drop(columns=[\"Id\"]).copy()\n",
    "\n",
    "# mêmes colonnes brutes\n",
    "assert list(X_base.columns) == list(X_test_base.columns), \"Colonnes train/test différentes !\"\n",
    "\n",
    "# FE identique sur train & test\n",
    "X = make_features(X_base)\n",
    "X_test = make_features(X_test_base)\n",
    "\n",
    "# sécurité : même ordre de colonnes après FE\n",
    "X = X.reindex(sorted(X.columns), axis=1)\n",
    "X_test = X_test.reindex(sorted(X_test.columns), axis=1)\n",
    "assert list(X.columns) == list(X_test.columns)\n",
    "\n",
    "# ============== 2) Split & First training (with ES) ==============\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X, y, test_size=VAL_SIZE, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=4000,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=256,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"multiclass\",\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=\"multi_logloss\",\n",
    "    callbacks=[early_stopping(EARLY_STOP_ROUNDS, verbose=True)]\n",
    ")\n",
    "\n",
    "# Accuracy de validation\n",
    "y_pred = model.predict(X_val, num_iteration=model.best_iteration_)\n",
    "val_acc = accuracy_score(y_val, y_pred)\n",
    "print(f\"\\nValidation Accuracy: {val_acc:.5f}\")\n",
    "\n",
    "best_iter = getattr(model, \"best_iteration_\", None)\n",
    "if best_iter is None:\n",
    "    best_iter = model.get_params().get(\"n_estimators\", 4000)\n",
    "print(f\"Best iteration retenue: {best_iter}\")\n",
    "\n",
    "# ============== 3) Retrain on FULL train with best_iter ==============\n",
    "final_model = LGBMClassifier(\n",
    "    n_estimators=best_iter,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=256,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"multiclass\",\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "final_model.fit(X, y)  # entraînement sur 100% des données sans early stopping\n",
    "\n",
    "# ============== 4) Prédictions test + fichiers ==============\n",
    "test_pred = final_model.predict(X_test).astype(int)\n",
    "submission = pd.DataFrame({\"Id\": test[\"Id\"], \"Cover_Type\": test_pred})\n",
    "\n",
    "csv_path = \"submission.csv\"\n",
    "zip_path = \"submission.csv.zip\"\n",
    "\n",
    "submission.to_csv(csv_path, index=False)\n",
    "print(f\"\\n✅ Fichier CSV créé: {csv_path}  | shape={submission.shape}\")\n",
    "\n",
    "# zip\n",
    "with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "    z.write(csv_path, arcname=os.path.basename(csv_path))\n",
    "print(f\"✅ Archive ZIP créée: {zip_path}\")\n",
    "\n",
    "# aperçu\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "503f5022-3dc3-405a-bbd9-a1f9e49ec845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2310]\tvalid_0's multi_logloss: 0.347883\n",
      "\n",
      "Validation Accuracy: 0.87368\n",
      "Best iteration selected: 2310\n",
      "\n",
      "✅ Created: submission.csv | shape=(581012, 2)\n",
      "✅ Created: submission.csv.zip\n",
      "   Id  Cover_Type\n",
      "0   1           5\n",
      "1   2           5\n",
      "2   3           2\n",
      "3   4           2\n",
      "4   5           5\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Forest Cover Type — Strong LGBM + ELU + TE + (opt) seed bagging\n",
    "# ============================\n",
    "import os, zipfile, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from lightgbm import LGBMClassifier, early_stopping\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "SEED = 42\n",
    "VAL_SIZE = 0.20\n",
    "EARLY_STOP_ROUNDS = 200\n",
    "USE_BAGGING = True\n",
    "BAGGING_SEEDS = [42, 123, 2024]  # used only for final test-time averaging\n",
    "\n",
    "# ----------------------------\n",
    "# Load\n",
    "# ----------------------------\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test  = pd.read_csv(\"test-full.csv\")\n",
    "\n",
    "y = train[\"Cover_Type\"].astype(int)\n",
    "X_base = train.drop(columns=[\"Cover_Type\", \"Id\"]).copy()\n",
    "X_test_base = test.drop(columns=[\"Id\"]).copy()\n",
    "\n",
    "assert list(X_base.columns) == list(X_test_base.columns), \"Train/Test columns mismatch!\"\n",
    "\n",
    "# ----------------------------\n",
    "# Feature Engineering\n",
    "# ----------------------------\n",
    "def make_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = df.copy()\n",
    "\n",
    "    # Aspect (wrap-around) -> trig\n",
    "    X[\"Aspect_sin\"] = np.sin(np.deg2rad(X[\"Aspect\"].astype(np.float32)))\n",
    "    X[\"Aspect_cos\"] = np.cos(np.deg2rad(X[\"Aspect\"].astype(np.float32)))\n",
    "\n",
    "    # Hydrology geometry\n",
    "    h_hyd = X[\"Horizontal_Distance_To_Hydrology\"].astype(np.float32)\n",
    "    v_hyd = X[\"Vertical_Distance_To_Hydrology\"].astype(np.float32)\n",
    "    X[\"Hydro_Dist\"] = np.hypot(h_hyd, v_hyd)\n",
    "    X[\"Abs_VertHydro\"] = np.abs(v_hyd)\n",
    "\n",
    "    # Distances combos\n",
    "    h_road = X[\"Horizontal_Distance_To_Roadways\"].astype(np.float32)\n",
    "    h_fire = X[\"Horizontal_Distance_To_Fire_Points\"].astype(np.float32)\n",
    "    X[\"Near_RoadOrFire\"] = np.minimum(h_road, h_fire)\n",
    "    X[\"Road_Fire\"] = h_road + h_fire\n",
    "    X[\"Hydro_Road\"] = h_hyd + h_road\n",
    "    X[\"Hydro_Fire\"] = h_hyd + h_fire\n",
    "    X[\"Road_Fire_Diff\"]  = np.abs(h_road - h_fire)\n",
    "    X[\"Road_Hydro_Diff\"] = np.abs(h_road - h_hyd)\n",
    "    X[\"Fire_Hydro_Diff\"] = np.abs(h_fire - h_hyd)\n",
    "\n",
    "    # Elevation interactions\n",
    "    elev = X[\"Elevation\"].astype(np.float32)\n",
    "    X[\"Elev_minus_VertHydro\"] = elev - v_hyd\n",
    "    X[\"Elev_plus_VertHydro\"]  = elev + v_hyd\n",
    "    X[\"VertHydro_over_Elev\"]  = v_hyd / (elev + 1e-3)\n",
    "\n",
    "    # Hillshade summaries\n",
    "    hs9 = X[\"Hillshade_9am\"].astype(np.float32)\n",
    "    hsn = X[\"Hillshade_Noon\"].astype(np.float32)\n",
    "    hs3 = X[\"Hillshade_3pm\"].astype(np.float32)\n",
    "    X[\"Hillshade_mean\"]  = (hs9 + hsn + hs3) / 3.0\n",
    "    X[\"Hillshade_range\"] = pd.concat([hs9, hsn, hs3], axis=1).max(axis=1) - \\\n",
    "                           pd.concat([hs9, hsn, hs3], axis=1).min(axis=1)\n",
    "    X[\"Hillshade_Noon_minus_9am\"] = hsn - hs9\n",
    "    X[\"Hillshade_3pm_minus_Noon\"] = hs3 - hsn\n",
    "\n",
    "    # Slope flags\n",
    "    X[\"Is_Flat\"]  = (X[\"Slope\"] == 0).astype(np.int8)\n",
    "    X[\"Is_Steep\"] = (X[\"Slope\"] >= 25).astype(np.int8)\n",
    "\n",
    "    # Collapse Soil/Wilderness one-hots -> single categories\n",
    "    soil_cols = [c for c in X.columns if c.startswith(\"Soil_Type\")]\n",
    "    wild_cols = [c for c in X.columns if c.startswith(\"Wilderness_Area\")]\n",
    "\n",
    "    X[\"Soil\"] = (X[soil_cols].values.argmax(axis=1) + 1).astype(np.int16)  # 1..40\n",
    "    X[\"Wilderness\"] = (X[wild_cols].values.argmax(axis=1) + 1).astype(np.int8)  # 1..4\n",
    "\n",
    "    # Drop original one-hots\n",
    "    X.drop(columns=soil_cols + wild_cols, inplace=True)\n",
    "\n",
    "    # Simple interactions\n",
    "    X[\"Elev_x_Soil\"]  = elev * X[\"Soil\"].astype(np.float32)\n",
    "    X[\"Slope_x_Soil\"] = X[\"Slope\"].astype(np.float32) * X[\"Soil\"].astype(np.float32)\n",
    "\n",
    "    return X\n",
    "\n",
    "X = make_features(X_base)\n",
    "X_test = make_features(X_test_base)\n",
    "\n",
    "# Soil -> ELU (climate & geology)\n",
    "soil_to_elu = {\n",
    " 1:2702, 2:2703, 3:2704, 4:2705, 5:2706, 6:2717, 7:3501, 8:3502, 9:4201, 10:4703,\n",
    " 11:4704, 12:4744, 13:4758, 14:5101, 15:5151, 16:6101, 17:6102, 18:6731, 19:7101, 20:7102,\n",
    " 21:7103, 22:7201, 23:7202, 24:7700, 25:7701, 26:7702, 27:7709, 28:7710, 29:7745, 30:7746,\n",
    " 31:7755, 32:7756, 33:7757, 34:7790, 35:8703, 36:8707, 37:8708, 38:8771, 39:8772, 40:8776\n",
    "}\n",
    "\n",
    "def add_elu_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"ELU_code\"]   = out[\"Soil\"].map(soil_to_elu).astype(int)\n",
    "    out[\"ELU_climate\"] = (out[\"ELU_code\"] // 1000).astype(np.int8)        # 1..8 (climate)\n",
    "    out[\"ELU_geo\"]     = ((out[\"ELU_code\"] // 100) % 10).astype(np.int8)   # 1..8 (geology)\n",
    "    out.drop(columns=[\"ELU_code\"], inplace=True)\n",
    "    return out\n",
    "\n",
    "X = add_elu_features(X)\n",
    "X_test = add_elu_features(X_test)\n",
    "\n",
    "# Tiny helpers for class 1 vs 2 separation\n",
    "X[\"Is_Climate_7_8\"] = X[\"ELU_climate\"].isin([7, 8]).astype(np.int8)\n",
    "X_test[\"Is_Climate_7_8\"] = X_test[\"ELU_climate\"].isin([7, 8]).astype(np.int8)\n",
    "X[\"Elev_x_Climate\"] = X[\"Elevation\"].astype(np.float32) * X[\"Is_Climate_7_8\"]\n",
    "X_test[\"Elev_x_Climate\"] = X_test[\"Elevation\"].astype(np.float32) * X_test[\"Is_Climate_7_8\"]\n",
    "\n",
    "# ----------------------------\n",
    "# Target Encoding (Multiclass) on Soil & Soil_Wild\n",
    "# ----------------------------\n",
    "def _fit_te_maps_multiclass(X, y, col, smoothing=8):\n",
    "    K = len(np.unique(y))\n",
    "    priors = y.value_counts(normalize=True).sort_index().values  # class priors 1..K\n",
    "    df = pd.DataFrame({col: X[col].values, \"y\": y.values})\n",
    "    tab = df.groupby(col)[\"y\"].value_counts().unstack(fill_value=0)\n",
    "    tab = tab.reindex(columns=range(1, K+1), fill_value=0)\n",
    "    cnts = tab.sum(axis=1).values[:, None]\n",
    "    probs = (tab.values + smoothing * priors) / (cnts + smoothing)\n",
    "    mapping = {k: probs[i] for i, k in enumerate(tab.index)}\n",
    "    return mapping, priors\n",
    "\n",
    "def cv_target_encode_multiclass_apply_to_test(X, y, X_test, cols, n_splits=5, seed=42, smoothing=8):\n",
    "    X = X.copy()\n",
    "    X_test = X_test.copy()\n",
    "    K = len(np.unique(y))\n",
    "    priors = y.value_counts(normalize=True).sort_index().values\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "    for col in cols:\n",
    "        # OOF encoding for X\n",
    "        oof = np.zeros((len(X), K), dtype=np.float32)\n",
    "        for tr, va in skf.split(X, y):\n",
    "            df_tr = pd.DataFrame({col: X.iloc[tr][col].values, \"y\": y.iloc[tr].values})\n",
    "            tab = df_tr.groupby(col)[\"y\"].value_counts().unstack(fill_value=0)\n",
    "            tab = tab.reindex(columns=range(1, K+1), fill_value=0)\n",
    "            cnts = tab.sum(axis=1).values[:, None]\n",
    "            probs = (tab.values + smoothing * priors) / (cnts + smoothing)\n",
    "            mapping = {k: probs[i] for i, k in enumerate(tab.index)}\n",
    "\n",
    "            enc = np.tile(priors, (len(va), 1))\n",
    "            keys = X.iloc[va][col].values\n",
    "            for j, key in enumerate(keys):\n",
    "                if key in mapping:\n",
    "                    enc[j] = mapping[key]\n",
    "            oof[va] = enc\n",
    "\n",
    "        for k in range(K):\n",
    "            X[f\"{col}_te_{k+1}\"] = oof[:, k]\n",
    "\n",
    "        # Full-train mapping for X_test\n",
    "        mapping_full, pri = _fit_te_maps_multiclass(X, y, col, smoothing=smoothing)\n",
    "        enc_test = np.tile(pri, (len(X_test), 1))\n",
    "        keys_t = X_test[col].values\n",
    "        for j, key in enumerate(keys_t):\n",
    "            if key in mapping_full:\n",
    "                enc_test[j] = mapping_full[key]\n",
    "        for k in range(K):\n",
    "            X_test[f\"{col}_te_{k+1}\"] = enc_test[:, k]\n",
    "\n",
    "    return X, X_test\n",
    "\n",
    "# Build Soil_Wild key (string ONLY for TE step)\n",
    "X[\"Soil_Wild\"] = X[\"Soil\"].astype(str) + \"_\" + X[\"Wilderness\"].astype(str)\n",
    "X_test[\"Soil_Wild\"] = X_test[\"Soil\"].astype(str) + \"_\" + X_test[\"Wilderness\"].astype(str)\n",
    "\n",
    "# Apply TE on Soil & Soil_Wild (lighter smoothing=8)\n",
    "X, X_test = cv_target_encode_multiclass_apply_to_test(\n",
    "    X, y, X_test,\n",
    "    cols=[\"Soil\", \"Soil_Wild\"],\n",
    "    n_splits=5, seed=SEED, smoothing=8\n",
    ")\n",
    "\n",
    "# Drop raw string key\n",
    "X.drop(columns=[\"Soil_Wild\"], inplace=True)\n",
    "X_test.drop(columns=[\"Soil_Wild\"], inplace=True)\n",
    "# Ensure no object columns remain\n",
    "obj_cols = list(X.select_dtypes(include=[\"object\"]).columns)\n",
    "assert len(obj_cols) == 0, f\"Object dtypes remain: {obj_cols}\"\n",
    "\n",
    "# Keep identical column order\n",
    "X = X.reindex(sorted(X.columns), axis=1)\n",
    "X_test = X_test.reindex(sorted(X_test.columns), axis=1)\n",
    "assert list(X.columns) == list(X_test.columns)\n",
    "\n",
    "# Categorical columns for LGBM\n",
    "cat_cols = [c for c in [\"Soil\", \"Wilderness\", \"ELU_climate\", \"ELU_geo\"] if c in X.columns]\n",
    "\n",
    "# ----------------------------\n",
    "# Train/Val split + Early Stopping\n",
    "# ----------------------------\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X, y, test_size=VAL_SIZE, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "base_params = dict(\n",
    "    objective=\"multiclass\",\n",
    "    learning_rate=0.03,\n",
    "    n_estimators=20000,   # large cap; ES will pick ~2–3k\n",
    "    num_leaves=256,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=80,\n",
    "    subsample=0.8,\n",
    "    subsample_freq=1,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.2,\n",
    "    reg_lambda=0.8,\n",
    "    extra_trees=True,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "model = LGBMClassifier(**base_params)\n",
    "model.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=\"multi_logloss\",\n",
    "    categorical_feature=cat_cols,\n",
    "    callbacks=[early_stopping(EARLY_STOP_ROUNDS, verbose=True)]\n",
    ")\n",
    "\n",
    "# Validation accuracy\n",
    "y_pred = model.predict(X_val, num_iteration=model.best_iteration_)\n",
    "val_acc = accuracy_score(y_val, y_pred)\n",
    "print(f\"\\nValidation Accuracy: {val_acc:.5f}\")\n",
    "\n",
    "best_iter = int(getattr(model, \"best_iteration_\", model.get_params().get(\"n_estimators\", 4000)))\n",
    "print(f\"Best iteration selected: {best_iter}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Final training on 100% + (optional) seed bagging for test\n",
    "# ----------------------------\n",
    "def fit_full_and_predict(seed):\n",
    "    params = dict(base_params)\n",
    "    params[\"random_state\"] = seed\n",
    "    params[\"n_estimators\"] = best_iter\n",
    "    clf = LGBMClassifier(**params)\n",
    "    clf.fit(X, y, categorical_feature=cat_cols)\n",
    "    proba = clf.predict_proba(X_test)  # (n,7)\n",
    "    return proba\n",
    "\n",
    "if USE_BAGGING:\n",
    "    proba_sum = np.zeros((len(X_test), 7), dtype=np.float32)\n",
    "    for s in BAGGING_SEEDS:\n",
    "        proba_sum += fit_full_and_predict(s)\n",
    "    proba = proba_sum / float(len(BAGGING_SEEDS))\n",
    "    test_pred = proba.argmax(axis=1) + 1\n",
    "else:\n",
    "    params = dict(base_params)\n",
    "    params[\"n_estimators\"] = best_iter\n",
    "    final_model = LGBMClassifier(**params)\n",
    "    final_model.fit(X, y, categorical_feature=cat_cols)\n",
    "    test_pred = final_model.predict(X_test)\n",
    "\n",
    "test_pred = test_pred.astype(int)\n",
    "submission = pd.DataFrame({\"Id\": test[\"Id\"], \"Cover_Type\": test_pred})\n",
    "\n",
    "# ----------------------------\n",
    "# Save CSV and ZIP\n",
    "# ----------------------------\n",
    "csv_path = \"submission.csv\"\n",
    "zip_path = \"submission.csv.zip\"\n",
    "submission.to_csv(csv_path, index=False)\n",
    "print(f\"\\n✅ Created: {csv_path} | shape={submission.shape}\")\n",
    "\n",
    "with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "    z.write(csv_path, arcname=os.path.basename(csv_path))\n",
    "print(f\"✅ Created: {zip_path}\")\n",
    "\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69a70f48-9edb-4761-a954-9cb099de2e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        pred_1  pred_2  pred_3  pred_4  pred_5  pred_6  pred_7\n",
      "true_1     337      63       0       0       5       0      27\n",
      "true_2      70     311      10       0      25      14       2\n",
      "true_3       0       0     378      13       4      37       0\n",
      "true_4       0       0      15     414       0       3       0\n",
      "true_5       5       9       5       0     411       2       0\n",
      "true_6       0       0      29       7       3     393       0\n",
      "true_7      24       1       0       0       0       0     407\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1      0.773     0.780     0.776       432\n",
      "           2      0.810     0.720     0.762       432\n",
      "           3      0.865     0.875     0.870       432\n",
      "           4      0.954     0.958     0.956       432\n",
      "           5      0.917     0.951     0.934       432\n",
      "           6      0.875     0.910     0.892       432\n",
      "           7      0.933     0.942     0.938       432\n",
      "\n",
      "    accuracy                          0.877      3024\n",
      "   macro avg      0.875     0.877     0.876      3024\n",
      "weighted avg      0.875     0.877     0.876      3024\n",
      "\n",
      "\n",
      "Acc par Wilderness:\n",
      " Wilderness\n",
      "2    0.8214\n",
      "1    0.8563\n",
      "3    0.8734\n",
      "4    0.9025\n",
      "Name: ok, dtype: float64\n",
      "\n",
      "Acc par Soil (top 10 difficiles):\n",
      " Soil\n",
      "25    0.0000\n",
      "9     0.0000\n",
      "11    0.7424\n",
      "34    0.7500\n",
      "31    0.7746\n",
      "24    0.7872\n",
      "32    0.7986\n",
      "29    0.8023\n",
      "33    0.8235\n",
      "18    0.8333\n",
      "Name: ok, dtype: float64\n",
      "\n",
      "Répartition Wilderness train/test:\n",
      "               train%     test%\n",
      "Wilderness                    \n",
      "1           0.235979  0.448865\n",
      "3           0.416799  0.436074\n",
      "4           0.309590  0.063627\n",
      "2           0.037632  0.051434\n",
      "\n",
      "Répartition Soil train/test (top 12):\n",
      "         train%     test%\n",
      "Soil                    \n",
      "29    0.086508  0.198356\n",
      "23    0.049074  0.099399\n",
      "32    0.043849  0.090392\n",
      "33    0.040939  0.077716\n",
      "22    0.021958  0.057439\n",
      "10    0.138624  0.056168\n",
      "30    0.048677  0.051927\n",
      "12    0.017196  0.051584\n",
      "31    0.020106  0.044175\n",
      "24    0.017526  0.036622\n",
      "13    0.033929  0.030001\n",
      "38    0.049206  0.026803\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "# 1) Matrice de confusion + rapport par classe\n",
    "cm = confusion_matrix(y_val, y_pred, labels=np.arange(1,8))\n",
    "print(pd.DataFrame(cm, index=[f\"true_{i}\" for i in range(1,8)],\n",
    "                      columns=[f\"pred_{i}\" for i in range(1,8)]))\n",
    "print(\"\\nReport:\\n\", classification_report(y_val, y_pred, digits=3))\n",
    "\n",
    "# 2) Accuracy par Wilderness et par Soil\n",
    "def group_acc(dfX, y_true, y_hat, col):\n",
    "    s = pd.DataFrame({col: dfX[col].values, \"ok\": (y_true.values==y_hat)}).groupby(col)[\"ok\"].mean()\n",
    "    return s.sort_values()\n",
    "\n",
    "print(\"\\nAcc par Wilderness:\\n\", group_acc(X_val, y_val, y_pred, \"Wilderness\").round(4))\n",
    "print(\"\\nAcc par Soil (top 10 difficiles):\\n\",\n",
    "      group_acc(X_val, y_val, y_pred, \"Soil\").sort_values().head(10).round(4))\n",
    "\n",
    "# 3) Drift train/test\n",
    "def freq(col, X_train_full, X_test_full, k=10):\n",
    "    a = X_train_full[col].value_counts(normalize=True)\n",
    "    b = X_test_full[col].value_counts(normalize=True)\n",
    "    df = pd.concat([a.rename(\"train%\"), b.rename(\"test%\")], axis=1).fillna(0).sort_values(\"test%\", ascending=False)\n",
    "    return df.head(k)\n",
    "\n",
    "print(\"\\nRépartition Wilderness train/test:\\n\", freq(\"Wilderness\", X, X_test))\n",
    "print(\"\\nRépartition Soil train/test (top 12):\\n\", freq(\"Soil\", X, X_test, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33e441aa-623a-4be6-b070-75ab52602ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2636]\tvalid_0's multi_logloss: 0.342656\n",
      "\n",
      "Validation Accuracy: 0.87665\n",
      "Best iteration retenue: 2636\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 150\u001b[39m\n\u001b[32m    148\u001b[39m proba = np.zeros((\u001b[38;5;28mlen\u001b[39m(X_test), \u001b[32m7\u001b[39m), dtype=np.float32)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m rs \u001b[38;5;129;01min\u001b[39;00m BAGGING_SEEDS:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     proba += \u001b[43mfit_full_and_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m proba /= \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mlen\u001b[39m(BAGGING_SEEDS))\n\u001b[32m    152\u001b[39m test_pred = proba.argmax(axis=\u001b[32m1\u001b[39m) + \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 145\u001b[39m, in \u001b[36mfit_full_and_proba\u001b[39m\u001b[34m(random_state)\u001b[39m\n\u001b[32m    143\u001b[39m clf = LGBMClassifier(**params)\n\u001b[32m    144\u001b[39m clf.fit(X, y, categorical_feature=cat_cols)\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\lightgbm\\sklearn.py:1627\u001b[39m, in \u001b[36mLGBMClassifier.predict_proba\u001b[39m\u001b[34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[39m\n\u001b[32m   1615\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict_proba\u001b[39m(\n\u001b[32m   1616\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1617\u001b[39m     X: _LGBM_ScikitMatrixLike,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1624\u001b[39m     **kwargs: Any,\n\u001b[32m   1625\u001b[39m ):\n\u001b[32m   1626\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Docstring is set after definition, using a template.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1627\u001b[39m     result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mraw_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1635\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1636\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1637\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m._objective) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (raw_score \u001b[38;5;129;01mor\u001b[39;00m pred_leaf \u001b[38;5;129;01mor\u001b[39;00m pred_contrib):\n\u001b[32m   1638\u001b[39m         _log_warning(\n\u001b[32m   1639\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCannot compute class probabilities or labels \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1640\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdue to the usage of customized objective function.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1641\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mReturning raw scores instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1642\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\lightgbm\\sklearn.py:1144\u001b[39m, in \u001b[36mLGBMModel.predict\u001b[39m\u001b[34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[39m\n\u001b[32m   1141\u001b[39m predict_params = _choose_param_value(\u001b[33m\"\u001b[39m\u001b[33mnum_threads\u001b[39m\u001b[33m\"\u001b[39m, predict_params, \u001b[38;5;28mself\u001b[39m.n_jobs)\n\u001b[32m   1142\u001b[39m predict_params[\u001b[33m\"\u001b[39m\u001b[33mnum_threads\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._process_n_jobs(predict_params[\u001b[33m\"\u001b[39m\u001b[33mnum_threads\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Booster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[union-attr]\u001b[39;49;00m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpredict_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\lightgbm\\basic.py:4767\u001b[39m, in \u001b[36mBooster.predict\u001b[39m\u001b[34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features, **kwargs)\u001b[39m\n\u001b[32m   4765\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4766\u001b[39m         num_iteration = -\u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m4767\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4768\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4769\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4770\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4771\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4772\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4773\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4774\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_has_header\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_has_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4775\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4776\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\lightgbm\\basic.py:1204\u001b[39m, in \u001b[36m_InnerPredictor.predict\u001b[39m\u001b[34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features)\u001b[39m\n\u001b[32m   1197\u001b[39m     preds, nrow = \u001b[38;5;28mself\u001b[39m.__pred_for_csc(\n\u001b[32m   1198\u001b[39m         csc=data,\n\u001b[32m   1199\u001b[39m         start_iteration=start_iteration,\n\u001b[32m   1200\u001b[39m         num_iteration=num_iteration,\n\u001b[32m   1201\u001b[39m         predict_type=predict_type,\n\u001b[32m   1202\u001b[39m     )\n\u001b[32m   1203\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np.ndarray):\n\u001b[32m-> \u001b[39m\u001b[32m1204\u001b[39m     preds, nrow = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pred_for_np2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m _is_pyarrow_table(data):\n\u001b[32m   1211\u001b[39m     preds, nrow = \u001b[38;5;28mself\u001b[39m.__pred_for_pyarrow_table(\n\u001b[32m   1212\u001b[39m         table=data,\n\u001b[32m   1213\u001b[39m         start_iteration=start_iteration,\n\u001b[32m   1214\u001b[39m         num_iteration=num_iteration,\n\u001b[32m   1215\u001b[39m         predict_type=predict_type,\n\u001b[32m   1216\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\lightgbm\\basic.py:1361\u001b[39m, in \u001b[36m_InnerPredictor.__pred_for_np2d\u001b[39m\u001b[34m(self, mat, start_iteration, num_iteration, predict_type)\u001b[39m\n\u001b[32m   1359\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m preds, nrow\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1361\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__inner_predict_np2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1364\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\lightgbm\\basic.py:1308\u001b[39m, in \u001b[36m_InnerPredictor.__inner_predict_np2d\u001b[39m\u001b[34m(self, mat, start_iteration, num_iteration, predict_type, preds)\u001b[39m\n\u001b[32m   1305\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mWrong length of pre-allocated predict array\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1306\u001b[39m out_num_preds = ctypes.c_int64(\u001b[32m0\u001b[39m)\n\u001b[32m   1307\u001b[39m _safe_call(\n\u001b[32m-> \u001b[39m\u001b[32m1308\u001b[39m     \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLGBM_BoosterPredictForMat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1309\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mptr_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtype_ptr_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1318\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_c_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpred_parameter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_num_preds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPOINTER\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_double\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m )\n\u001b[32m   1323\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_preds != out_num_preds.value:\n\u001b[32m   1324\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mWrong length for predict results\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from lightgbm import LGBMClassifier, early_stopping\n",
    "import zipfile, os, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===================== Config =====================\n",
    "SEED = 42\n",
    "VAL_SIZE = 0.20\n",
    "EARLY_STOP_ROUNDS = 200\n",
    "\n",
    "# Small, stable test-time boost (bagging)\n",
    "USE_BAGGING = True\n",
    "BAGGING_SEEDS = [42, 123, 2024]   # keep small; averaging probabilities\n",
    "\n",
    "# ===================== Features (your best set) =====================\n",
    "def make_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = df.copy()\n",
    "\n",
    "    # Aspect trig\n",
    "    X[\"Aspect_sin\"] = np.sin(np.deg2rad(X[\"Aspect\"].astype(np.float32)))\n",
    "    X[\"Aspect_cos\"] = np.cos(np.deg2rad(X[\"Aspect\"].astype(np.float32)))\n",
    "\n",
    "    # Hydrology geometry\n",
    "    h_hyd = X[\"Horizontal_Distance_To_Hydrology\"].astype(np.float32)\n",
    "    v_hyd = X[\"Vertical_Distance_To_Hydrology\"].astype(np.float32)\n",
    "    X[\"Hydro_Dist\"] = np.hypot(h_hyd, v_hyd)\n",
    "    X[\"Abs_VertHydro\"] = np.abs(v_hyd)\n",
    "\n",
    "    # Road / Fire combos\n",
    "    h_road = X[\"Horizontal_Distance_To_Roadways\"].astype(np.float32)\n",
    "    h_fire = X[\"Horizontal_Distance_To_Fire_Points\"].astype(np.float32)\n",
    "    X[\"Near_RoadOrFire\"] = np.minimum(h_road, h_fire)\n",
    "    X[\"Road_Fire\"] = (h_road + h_fire)\n",
    "    X[\"Hydro_Road\"] = (h_hyd + h_road)\n",
    "    X[\"Hydro_Fire\"] = (h_hyd + h_fire)\n",
    "\n",
    "    # Useful diffs\n",
    "    X[\"Road_Fire_Diff\"]  = np.abs(h_road - h_fire)\n",
    "    X[\"Road_Hydro_Diff\"] = np.abs(h_road - h_hyd)\n",
    "    X[\"Fire_Hydro_Diff\"] = np.abs(h_fire - h_hyd)\n",
    "\n",
    "    # Elevation interactions\n",
    "    elev = X[\"Elevation\"].astype(np.float32)\n",
    "    X[\"Elev_minus_VertHydro\"] = elev - v_hyd\n",
    "    X[\"Elev_plus_VertHydro\"]  = elev + v_hyd\n",
    "    X[\"VertHydro_over_Elev\"]  = v_hyd / (elev + 1e-3)\n",
    "\n",
    "    # Hillshade summaries + deltas\n",
    "    hs9  = X[\"Hillshade_9am\"].astype(np.float32)\n",
    "    hsn  = X[\"Hillshade_Noon\"].astype(np.float32)\n",
    "    hs3  = X[\"Hillshade_3pm\"].astype(np.float32)\n",
    "    X[\"Hillshade_mean\"]  = (hs9 + hsn + hs3) / 3.0\n",
    "    X[\"Hillshade_range\"] = pd.concat([hs9, hsn, hs3], axis=1).max(axis=1) - \\\n",
    "                           pd.concat([hs9, hsn, hs3], axis=1).min(axis=1)\n",
    "    X[\"Hillshade_Noon_minus_9am\"] = hsn - hs9\n",
    "    X[\"Hillshade_3pm_minus_Noon\"] = hs3 - hsn\n",
    "\n",
    "    # Slope flags\n",
    "    X[\"Is_Flat\"]  = (X[\"Slope\"] == 0).astype(np.int8)\n",
    "    X[\"Is_Steep\"] = (X[\"Slope\"] >= 25).astype(np.int8)\n",
    "\n",
    "    # Collapse Soil/Wilderness one-hots\n",
    "    soil_cols = [c for c in X.columns if c.startswith(\"Soil_Type\")]\n",
    "    wild_cols = [c for c in X.columns if c.startswith(\"Wilderness_Area\")]\n",
    "    X[\"Soil\"] = (X[soil_cols].values.argmax(axis=1) + 1).astype(np.int16)  # 1..40\n",
    "    X[\"Wilderness\"] = (X[wild_cols].values.argmax(axis=1) + 1).astype(np.int8)  # 1..4\n",
    "    X.drop(columns=soil_cols + wild_cols, inplace=True)\n",
    "\n",
    "    # Light interactions\n",
    "    X[\"Elev_x_Soil\"]  = elev * X[\"Soil\"].astype(np.float32)\n",
    "    X[\"Slope_x_Soil\"] = X[\"Slope\"].astype(np.float32) * X[\"Soil\"].astype(np.float32)\n",
    "    return X\n",
    "\n",
    "# ===================== Load =====================\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test  = pd.read_csv(\"test-full.csv\")\n",
    "\n",
    "y = train[\"Cover_Type\"].astype(int)\n",
    "X_base = train.drop(columns=[\"Cover_Type\", \"Id\"]).copy()\n",
    "X_test_base = test.drop(columns=[\"Id\"]).copy()\n",
    "assert list(X_base.columns) == list(X_test_base.columns), \"Colonnes train/test différentes !\"\n",
    "\n",
    "# FE identique\n",
    "X = make_features(X_base)\n",
    "X_test = make_features(X_test_base)\n",
    "\n",
    "# sécurité : même ordre de colonnes après FE\n",
    "X = X.reindex(sorted(X.columns), axis=1)\n",
    "X_test = X_test.reindex(sorted(X_test.columns), axis=1)\n",
    "assert list(X.columns) == list(X_test.columns)\n",
    "\n",
    "# colonnes catégorielles (noms) pour LGBM\n",
    "cat_cols = [c for c in [\"Soil\", \"Wilderness\"] if c in X.columns]\n",
    "\n",
    "# ===================== Split & First training (get best_iter) =====================\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X, y, test_size=VAL_SIZE, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "base_params = dict(\n",
    "    objective=\"multiclass\",\n",
    "    learning_rate=0.03,\n",
    "    n_estimators=20000,          # early stopping chooses\n",
    "    num_leaves=256,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=80,\n",
    "    subsample=0.8,\n",
    "    subsample_freq=1,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.2,\n",
    "    reg_lambda=0.8,\n",
    "    extra_trees=True,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "model = LGBMClassifier(**base_params)\n",
    "model.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=\"multi_logloss\",\n",
    "    categorical_feature=cat_cols,   # LGBM >= 4 accepts names\n",
    "    callbacks=[early_stopping(EARLY_STOP_ROUNDS, verbose=True)]\n",
    ")\n",
    "\n",
    "# Accuracy de validation\n",
    "y_pred = model.predict(X_val, num_iteration=model.best_iteration_)\n",
    "val_acc = accuracy_score(y_val, y_pred)\n",
    "print(f\"\\nValidation Accuracy: {val_acc:.5f}\")\n",
    "\n",
    "best_iter = int(getattr(model, \"best_iteration_\", model.get_params().get(\"n_estimators\", 4000)))\n",
    "print(f\"Best iteration retenue: {best_iter}\")\n",
    "\n",
    "# ===================== Final training on 100% + (opt) seed bagging =====================\n",
    "def fit_full_and_proba(random_state):\n",
    "    params = dict(base_params)\n",
    "    params[\"n_estimators\"] = best_iter\n",
    "    params[\"random_state\"] = random_state\n",
    "    clf = LGBMClassifier(**params)\n",
    "    clf.fit(X, y, categorical_feature=cat_cols)\n",
    "    return clf.predict_proba(X_test)  # (n_test, 7)\n",
    "\n",
    "if USE_BAGGING:\n",
    "    proba = np.zeros((len(X_test), 7), dtype=np.float32)\n",
    "    for rs in BAGGING_SEEDS:\n",
    "        proba += fit_full_and_proba(rs)\n",
    "    proba /= float(len(BAGGING_SEEDS))\n",
    "    test_pred = proba.argmax(axis=1) + 1\n",
    "else:\n",
    "    params = dict(base_params)\n",
    "    params[\"n_estimators\"] = best_iter\n",
    "    final_model = LGBMClassifier(**params)\n",
    "    final_model.fit(X, y, categorical_feature=cat_cols)\n",
    "    test_pred = final_model.predict(X_test)\n",
    "\n",
    "test_pred = test_pred.astype(int)\n",
    "submission = pd.DataFrame({\"Id\": test[\"Id\"], \"Cover_Type\": test_pred})\n",
    "\n",
    "# ===================== Save CSV & ZIP =====================\n",
    "csv_path = \"submission.csv\"\n",
    "zip_path = \"submission.csv.zip\"\n",
    "\n",
    "submission.to_csv(csv_path, index=False)\n",
    "print(f\"\\n✅ Fichier CSV créé: {csv_path} | shape={submission.shape}\")\n",
    "\n",
    "with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "    z.write(csv_path, arcname=os.path.basename(csv_path))\n",
    "print(f\"✅ Archive ZIP créée: {zip_path}\")\n",
    "\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9155786-04eb-48b4-a4c3-0a3f93447864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2636]\tvalid_0's multi_logloss: 0.342656\n",
      "\n",
      "Validation Accuracy: 0.87665\n",
      "Best iteration retenue: 2636\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 171\u001b[39m\n\u001b[32m    169\u001b[39m proba_sum = np.zeros((\u001b[38;5;28mlen\u001b[39m(X_test), \u001b[32m7\u001b[39m), dtype=np.float32)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (rs, leaves, mcs, subs, cols, lam) \u001b[38;5;129;01min\u001b[39;00m ENSEMBLE_SPECS:\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     proba_sum += \u001b[43mfit_variant_and_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleaves\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m proba = proba_sum / \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ENSEMBLE_SPECS))\n\u001b[32m    173\u001b[39m test_pred = proba.argmax(axis=\u001b[32m1\u001b[39m) + \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 158\u001b[39m, in \u001b[36mfit_variant_and_proba\u001b[39m\u001b[34m(rs, leaves, mcs, subs, cols, lam)\u001b[39m\n\u001b[32m    140\u001b[39m clf = LGBMClassifier(\n\u001b[32m    141\u001b[39m     objective=\u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    142\u001b[39m     learning_rate=\u001b[32m0.03\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    155\u001b[39m     verbosity=-\u001b[32m1\u001b[39m\n\u001b[32m    156\u001b[39m )\n\u001b[32m    157\u001b[39m clf.fit(X, y, categorical_feature=cat_cols)\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\lightgbm\\sklearn.py:1627\u001b[39m, in \u001b[36mLGBMClassifier.predict_proba\u001b[39m\u001b[34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[39m\n\u001b[32m   1615\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict_proba\u001b[39m(\n\u001b[32m   1616\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1617\u001b[39m     X: _LGBM_ScikitMatrixLike,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1624\u001b[39m     **kwargs: Any,\n\u001b[32m   1625\u001b[39m ):\n\u001b[32m   1626\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Docstring is set after definition, using a template.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1627\u001b[39m     result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mraw_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1635\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1636\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1637\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m._objective) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (raw_score \u001b[38;5;129;01mor\u001b[39;00m pred_leaf \u001b[38;5;129;01mor\u001b[39;00m pred_contrib):\n\u001b[32m   1638\u001b[39m         _log_warning(\n\u001b[32m   1639\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCannot compute class probabilities or labels \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1640\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdue to the usage of customized objective function.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1641\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mReturning raw scores instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1642\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\lightgbm\\sklearn.py:1144\u001b[39m, in \u001b[36mLGBMModel.predict\u001b[39m\u001b[34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[39m\n\u001b[32m   1141\u001b[39m predict_params = _choose_param_value(\u001b[33m\"\u001b[39m\u001b[33mnum_threads\u001b[39m\u001b[33m\"\u001b[39m, predict_params, \u001b[38;5;28mself\u001b[39m.n_jobs)\n\u001b[32m   1142\u001b[39m predict_params[\u001b[33m\"\u001b[39m\u001b[33mnum_threads\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._process_n_jobs(predict_params[\u001b[33m\"\u001b[39m\u001b[33mnum_threads\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Booster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[union-attr]\u001b[39;49;00m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpredict_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\lightgbm\\basic.py:4767\u001b[39m, in \u001b[36mBooster.predict\u001b[39m\u001b[34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features, **kwargs)\u001b[39m\n\u001b[32m   4765\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4766\u001b[39m         num_iteration = -\u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m4767\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4768\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4769\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4770\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4771\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4772\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4773\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4774\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_has_header\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_has_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4775\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4776\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\lightgbm\\basic.py:1204\u001b[39m, in \u001b[36m_InnerPredictor.predict\u001b[39m\u001b[34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features)\u001b[39m\n\u001b[32m   1197\u001b[39m     preds, nrow = \u001b[38;5;28mself\u001b[39m.__pred_for_csc(\n\u001b[32m   1198\u001b[39m         csc=data,\n\u001b[32m   1199\u001b[39m         start_iteration=start_iteration,\n\u001b[32m   1200\u001b[39m         num_iteration=num_iteration,\n\u001b[32m   1201\u001b[39m         predict_type=predict_type,\n\u001b[32m   1202\u001b[39m     )\n\u001b[32m   1203\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np.ndarray):\n\u001b[32m-> \u001b[39m\u001b[32m1204\u001b[39m     preds, nrow = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pred_for_np2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m _is_pyarrow_table(data):\n\u001b[32m   1211\u001b[39m     preds, nrow = \u001b[38;5;28mself\u001b[39m.__pred_for_pyarrow_table(\n\u001b[32m   1212\u001b[39m         table=data,\n\u001b[32m   1213\u001b[39m         start_iteration=start_iteration,\n\u001b[32m   1214\u001b[39m         num_iteration=num_iteration,\n\u001b[32m   1215\u001b[39m         predict_type=predict_type,\n\u001b[32m   1216\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\lightgbm\\basic.py:1361\u001b[39m, in \u001b[36m_InnerPredictor.__pred_for_np2d\u001b[39m\u001b[34m(self, mat, start_iteration, num_iteration, predict_type)\u001b[39m\n\u001b[32m   1359\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m preds, nrow\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1361\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__inner_predict_np2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1364\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\lightgbm\\basic.py:1308\u001b[39m, in \u001b[36m_InnerPredictor.__inner_predict_np2d\u001b[39m\u001b[34m(self, mat, start_iteration, num_iteration, predict_type, preds)\u001b[39m\n\u001b[32m   1305\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mWrong length of pre-allocated predict array\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1306\u001b[39m out_num_preds = ctypes.c_int64(\u001b[32m0\u001b[39m)\n\u001b[32m   1307\u001b[39m _safe_call(\n\u001b[32m-> \u001b[39m\u001b[32m1308\u001b[39m     \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLGBM_BoosterPredictForMat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1309\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mptr_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtype_ptr_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1318\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_c_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpred_parameter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_num_preds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPOINTER\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_double\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m )\n\u001b[32m   1323\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_preds != out_num_preds.value:\n\u001b[32m   1324\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mWrong length for predict results\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from lightgbm import LGBMClassifier, early_stopping\n",
    "import zipfile, os, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===================== Config =====================\n",
    "SEED = 42\n",
    "VAL_SIZE = 0.20\n",
    "EARLY_STOP_ROUNDS = 200\n",
    "\n",
    "# Toggle simple seed bagging or a slightly stronger variant ensemble\n",
    "USE_VARIANT_ENSEMBLE = True   # set False to use simple bagging with same params\n",
    "BAGGING_SEEDS = [42, 123, 2024]\n",
    "\n",
    "# ===================== Features (your best set) =====================\n",
    "def make_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = df.copy()\n",
    "\n",
    "    # Aspect trig\n",
    "    X[\"Aspect_sin\"] = np.sin(np.deg2rad(X[\"Aspect\"].astype(np.float32)))\n",
    "    X[\"Aspect_cos\"] = np.cos(np.deg2rad(X[\"Aspect\"].astype(np.float32)))\n",
    "\n",
    "    # Hydrology geometry\n",
    "    h_hyd = X[\"Horizontal_Distance_To_Hydrology\"].astype(np.float32)\n",
    "    v_hyd = X[\"Vertical_Distance_To_Hydrology\"].astype(np.float32)\n",
    "    X[\"Hydro_Dist\"] = np.hypot(h_hyd, v_hyd)\n",
    "    X[\"Abs_VertHydro\"] = np.abs(v_hyd)\n",
    "\n",
    "    # Road / Fire combos\n",
    "    h_road = X[\"Horizontal_Distance_To_Roadways\"].astype(np.float32)\n",
    "    h_fire = X[\"Horizontal_Distance_To_Fire_Points\"].astype(np.float32)\n",
    "    X[\"Near_RoadOrFire\"] = np.minimum(h_road, h_fire)\n",
    "    X[\"Road_Fire\"] = (h_road + h_fire)\n",
    "    X[\"Hydro_Road\"] = (h_hyd + h_road)\n",
    "    X[\"Hydro_Fire\"] = (h_hyd + h_fire)\n",
    "\n",
    "    # Useful diffs\n",
    "    X[\"Road_Fire_Diff\"]  = np.abs(h_road - h_fire)\n",
    "    X[\"Road_Hydro_Diff\"] = np.abs(h_road - h_hyd)\n",
    "    X[\"Fire_Hydro_Diff\"] = np.abs(h_fire - h_hyd)\n",
    "\n",
    "    # Elevation interactions\n",
    "    elev = X[\"Elevation\"].astype(np.float32)\n",
    "    X[\"Elev_minus_VertHydro\"] = elev - v_hyd\n",
    "    X[\"Elev_plus_VertHydro\"]  = elev + v_hyd\n",
    "    X[\"VertHydro_over_Elev\"]  = v_hyd / (elev + 1e-3)\n",
    "\n",
    "    # Hillshade summaries + deltas\n",
    "    hs9  = X[\"Hillshade_9am\"].astype(np.float32)\n",
    "    hsn  = X[\"Hillshade_Noon\"].astype(np.float32)\n",
    "    hs3  = X[\"Hillshade_3pm\"].astype(np.float32)\n",
    "    X[\"Hillshade_mean\"]  = (hs9 + hsn + hs3) / 3.0\n",
    "    X[\"Hillshade_range\"] = pd.concat([hs9, hsn, hs3], axis=1).max(axis=1) - \\\n",
    "                           pd.concat([hs9, hsn, hs3], axis=1).min(axis=1)\n",
    "    X[\"Hillshade_Noon_minus_9am\"] = hsn - hs9\n",
    "    X[\"Hillshade_3pm_minus_Noon\"] = hs3 - hsn\n",
    "\n",
    "    # Slope flags\n",
    "    X[\"Is_Flat\"]  = (X[\"Slope\"] == 0).astype(np.int8)\n",
    "    X[\"Is_Steep\"] = (X[\"Slope\"] >= 25).astype(np.int8)\n",
    "\n",
    "    # Collapse Soil/Wilderness one-hots\n",
    "    soil_cols = [c for c in X.columns if c.startswith(\"Soil_Type\")]\n",
    "    wild_cols = [c for c in X.columns if c.startswith(\"Wilderness_Area\")]\n",
    "    X[\"Soil\"] = (X[soil_cols].values.argmax(axis=1) + 1).astype(np.int16)  # 1..40\n",
    "    X[\"Wilderness\"] = (X[wild_cols].values.argmax(axis=1) + 1).astype(np.int8)  # 1..4\n",
    "    X.drop(columns=soil_cols + wild_cols, inplace=True)\n",
    "\n",
    "    # Light interactions\n",
    "    X[\"Elev_x_Soil\"]  = elev * X[\"Soil\"].astype(np.float32)\n",
    "    X[\"Slope_x_Soil\"] = X[\"Slope\"].astype(np.float32) * X[\"Soil\"].astype(np.float32)\n",
    "    return X\n",
    "\n",
    "# ===================== Load =====================\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test  = pd.read_csv(\"test-full.csv\")\n",
    "\n",
    "y = train[\"Cover_Type\"].astype(int)\n",
    "X_base = train.drop(columns=[\"Cover_Type\", \"Id\"]).copy()\n",
    "X_test_base = test.drop(columns=[\"Id\"]).copy()\n",
    "assert list(X_base.columns) == list(X_test_base.columns), \"Colonnes train/test différentes !\"\n",
    "\n",
    "# FE identique\n",
    "X = make_features(X_base)\n",
    "X_test = make_features(X_test_base)\n",
    "\n",
    "# sécurité : même ordre de colonnes après FE\n",
    "X = X.reindex(sorted(X.columns), axis=1)\n",
    "X_test = X_test.reindex(sorted(X_test.columns), axis=1)\n",
    "assert list(X.columns) == list(X_test.columns)\n",
    "\n",
    "# colonnes catégorielles (noms) pour LGBM\n",
    "cat_cols = [c for c in [\"Soil\", \"Wilderness\"] if c in X.columns]\n",
    "\n",
    "# ===================== Split & First training (get best_iter) =====================\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X, y, test_size=VAL_SIZE, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "base_params = dict(\n",
    "    objective=\"multiclass\",\n",
    "    learning_rate=0.03,\n",
    "    n_estimators=20000,          # early stopping chooses\n",
    "    num_leaves=256,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=80,\n",
    "    subsample=0.8,\n",
    "    subsample_freq=1,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.2,\n",
    "    reg_lambda=0.8,\n",
    "    extra_trees=True,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "model = LGBMClassifier(**base_params)\n",
    "model.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=\"multi_logloss\",\n",
    "    categorical_feature=cat_cols,   # LGBM >= 4 accepts names\n",
    "    callbacks=[early_stopping(EARLY_STOP_ROUNDS, verbose=True)]\n",
    ")\n",
    "\n",
    "# Accuracy de validation\n",
    "y_pred = model.predict(X_val, num_iteration=model.best_iteration_)\n",
    "val_acc = accuracy_score(y_val, y_pred)\n",
    "print(f\"\\nValidation Accuracy: {val_acc:.5f}\")\n",
    "\n",
    "best_iter = int(getattr(model, \"best_iteration_\", model.get_params().get(\"n_estimators\", 4000)))\n",
    "print(f\"Best iteration retenue: {best_iter}\")\n",
    "\n",
    "# ===================== Final training on 100% with a small robust ensemble =====================\n",
    "def fit_variant_and_proba(rs, leaves, mcs, subs, cols, lam):\n",
    "    clf = LGBMClassifier(\n",
    "        objective=\"multiclass\",\n",
    "        learning_rate=0.03,\n",
    "        n_estimators=best_iter,     # reuse best_iter from ES\n",
    "        num_leaves=leaves,\n",
    "        max_depth=-1,\n",
    "        min_child_samples=mcs,\n",
    "        subsample=subs,\n",
    "        subsample_freq=1,\n",
    "        colsample_bytree=cols,\n",
    "        reg_alpha=0.2,\n",
    "        reg_lambda=lam,\n",
    "        extra_trees=True,\n",
    "        random_state=rs,\n",
    "        n_jobs=-1,\n",
    "        verbosity=-1\n",
    "    )\n",
    "    clf.fit(X, y, categorical_feature=cat_cols)\n",
    "    return clf.predict_proba(X_test)  # (n_test, 7)\n",
    "\n",
    "if USE_VARIANT_ENSEMBLE:\n",
    "    ENSEMBLE_SPECS = [\n",
    "        # (random_state, num_leaves, min_child_samples, subsample, colsample_bytree, reg_lambda)\n",
    "        (42,   256, 80,  0.80, 0.80, 0.8),\n",
    "        (123,  224, 100, 0.75, 0.75, 1.0),\n",
    "        (2024, 288, 80,  0.80, 0.75, 0.8),\n",
    "        (7,    256, 100, 0.75, 0.80, 1.0),\n",
    "        (314,  224, 120, 0.75, 0.75, 1.2),\n",
    "    ]\n",
    "    proba_sum = np.zeros((len(X_test), 7), dtype=np.float32)\n",
    "    for (rs, leaves, mcs, subs, cols, lam) in ENSEMBLE_SPECS:\n",
    "        proba_sum += fit_variant_and_proba(rs, leaves, mcs, subs, cols, lam)\n",
    "    proba = proba_sum / float(len(ENSEMBLE_SPECS))\n",
    "    test_pred = proba.argmax(axis=1) + 1\n",
    "else:\n",
    "    # simple seed bagging with identical params\n",
    "    proba_sum = np.zeros((len(X_test), 7), dtype=np.float32)\n",
    "    for rs in BAGGING_SEEDS:\n",
    "        proba_sum += fit_variant_and_proba(rs, 256, 80, 0.80, 0.80, 0.8)\n",
    "    proba = proba_sum / float(len(BAGGING_SEEDS))\n",
    "    test_pred = proba.argmax(axis=1) + 1\n",
    "\n",
    "test_pred = test_pred.astype(int)\n",
    "submission = pd.DataFrame({\"Id\": test[\"Id\"], \"Cover_Type\": test_pred})\n",
    "\n",
    "# ===================== Save CSV & ZIP =====================\n",
    "csv_path = \"submission.csv\"\n",
    "zip_path = \"submission.csv.zip\"\n",
    "\n",
    "submission.to_csv(csv_path, index=False)\n",
    "print(f\"\\n✅ Fichier CSV créé: {csv_path} | shape={submission.shape}\")\n",
    "\n",
    "with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "    z.write(csv_path, arcname=os.path.basename(csv_path))\n",
    "print(f\"✅ Archive ZIP créée: {zip_path}\")\n",
    "\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95424b4f-218e-4a0f-842d-a6b08b14e422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2648]\tvalid_0's multi_logloss: 0.332692\n",
      "Fold 1: acc=0.87698 | best_iter=2648\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2918]\tvalid_0's multi_logloss: 0.314935\n",
      "Fold 2: acc=0.88988 | best_iter=2918\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2457]\tvalid_0's multi_logloss: 0.326914\n",
      "Fold 3: acc=0.87500 | best_iter=2457\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2918]\tvalid_0's multi_logloss: 0.320054\n",
      "Fold 4: acc=0.88128 | best_iter=2918\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2388]\tvalid_0's multi_logloss: 0.371053\n",
      "Fold 5: acc=0.86872 | best_iter=2388\n",
      "\n",
      "CV mean acc: 0.87837 ± 0.00703\n",
      "OOF acc    : 0.87837\n",
      "Best iters : [2648, 2918, 2457, 2918, 2388] | mean=2665\n",
      "\n",
      "✅ Fichier CSV créé: submission.csv | shape=(581012, 2)\n",
      "✅ Archive ZIP créée: submission.csv.zip\n",
      "   Id  Cover_Type\n",
      "0   1           5\n",
      "1   2           5\n",
      "2   3           2\n",
      "3   4           2\n",
      "4   5           5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from lightgbm import LGBMClassifier, early_stopping\n",
    "import zipfile, os, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===================== Config =====================\n",
    "SEED = 42\n",
    "N_FOLDS = 5\n",
    "EARLY_STOP_ROUNDS = 200\n",
    "\n",
    "# ===================== Features (your best set, unchanged) =====================\n",
    "def make_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = df.copy()\n",
    "\n",
    "    # Aspect trig\n",
    "    X[\"Aspect_sin\"] = np.sin(np.deg2rad(X[\"Aspect\"].astype(np.float32)))\n",
    "    X[\"Aspect_cos\"] = np.cos(np.deg2rad(X[\"Aspect\"].astype(np.float32)))\n",
    "\n",
    "    # Hydrology geometry\n",
    "    h_hyd = X[\"Horizontal_Distance_To_Hydrology\"].astype(np.float32)\n",
    "    v_hyd = X[\"Vertical_Distance_To_Hydrology\"].astype(np.float32)\n",
    "    X[\"Hydro_Dist\"] = np.hypot(h_hyd, v_hyd)\n",
    "    X[\"Abs_VertHydro\"] = np.abs(v_hyd)\n",
    "\n",
    "    # Road / Fire combos\n",
    "    h_road = X[\"Horizontal_Distance_To_Roadways\"].astype(np.float32)\n",
    "    h_fire = X[\"Horizontal_Distance_To_Fire_Points\"].astype(np.float32)\n",
    "    X[\"Near_RoadOrFire\"] = np.minimum(h_road, h_fire)\n",
    "    X[\"Road_Fire\"] = (h_road + h_fire)\n",
    "    X[\"Hydro_Road\"] = (h_hyd + h_road)\n",
    "    X[\"Hydro_Fire\"] = (h_hyd + h_fire)\n",
    "\n",
    "    # Useful diffs\n",
    "    X[\"Road_Fire_Diff\"]  = np.abs(h_road - h_fire)\n",
    "    X[\"Road_Hydro_Diff\"] = np.abs(h_road - h_hyd)\n",
    "    X[\"Fire_Hydro_Diff\"] = np.abs(h_fire - h_hyd)\n",
    "\n",
    "    # Elevation interactions\n",
    "    elev = X[\"Elevation\"].astype(np.float32)\n",
    "    X[\"Elev_minus_VertHydro\"] = elev - v_hyd\n",
    "    X[\"Elev_plus_VertHydro\"]  = elev + v_hyd\n",
    "    X[\"VertHydro_over_Elev\"]  = v_hyd / (elev + 1e-3)\n",
    "\n",
    "    # Hillshade summaries + deltas\n",
    "    hs9  = X[\"Hillshade_9am\"].astype(np.float32)\n",
    "    hsn  = X[\"Hillshade_Noon\"].astype(np.float32)\n",
    "    hs3  = X[\"Hillshade_3pm\"].astype(np.float32)\n",
    "    X[\"Hillshade_mean\"]  = (hs9 + hsn + hs3) / 3.0\n",
    "    X[\"Hillshade_range\"] = pd.concat([hs9, hsn, hs3], axis=1).max(axis=1) - \\\n",
    "                           pd.concat([hs9, hsn, hs3], axis=1).min(axis=1)\n",
    "    X[\"Hillshade_Noon_minus_9am\"] = hsn - hs9\n",
    "    X[\"Hillshade_3pm_minus_Noon\"] = hs3 - hsn\n",
    "\n",
    "    # Slope flags\n",
    "    X[\"Is_Flat\"]  = (X[\"Slope\"] == 0).astype(np.int8)\n",
    "    X[\"Is_Steep\"] = (X[\"Slope\"] >= 25).astype(np.int8)\n",
    "\n",
    "    # Collapse Soil/Wilderness one-hots\n",
    "    soil_cols = [c for c in X.columns if c.startswith(\"Soil_Type\")]\n",
    "    wild_cols = [c for c in X.columns if c.startswith(\"Wilderness_Area\")]\n",
    "    X[\"Soil\"] = (X[soil_cols].values.argmax(axis=1) + 1).astype(np.int16)  # 1..40\n",
    "    X[\"Wilderness\"] = (X[wild_cols].values.argmax(axis=1) + 1).astype(np.int8)  # 1..4\n",
    "    X.drop(columns=soil_cols + wild_cols, inplace=True)\n",
    "\n",
    "    # Light interactions\n",
    "    X[\"Elev_x_Soil\"]  = elev * X[\"Soil\"].astype(np.float32)\n",
    "    X[\"Slope_x_Soil\"] = X[\"Slope\"].astype(np.float32) * X[\"Soil\"].astype(np.float32)\n",
    "    return X\n",
    "\n",
    "# ===================== Load =====================\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test  = pd.read_csv(\"test-full.csv\")\n",
    "\n",
    "y = train[\"Cover_Type\"].astype(int)\n",
    "X_base = train.drop(columns=[\"Cover_Type\", \"Id\"]).copy()\n",
    "X_test_base = test.drop(columns=[\"Id\"]).copy()\n",
    "assert list(X_base.columns) == list(X_test_base.columns), \"Colonnes train/test différentes !\"\n",
    "\n",
    "# FE identique\n",
    "X = make_features(X_base)\n",
    "X_test = make_features(X_test_base)\n",
    "\n",
    "# sécurité : même ordre de colonnes après FE\n",
    "X = X.reindex(sorted(X.columns), axis=1)\n",
    "X_test = X_test.reindex(sorted(X_test.columns), axis=1)\n",
    "assert list(X.columns) == list(X_test.columns)\n",
    "\n",
    "# Categoricals for LGBM\n",
    "cat_cols = [c for c in [\"Soil\", \"Wilderness\"] if c in X.columns]\n",
    "\n",
    "# ===================== 5-fold CV with early stopping (same params) =====================\n",
    "base_params = dict(\n",
    "    objective=\"multiclass\",\n",
    "    learning_rate=0.03,\n",
    "    n_estimators=20000,          # ES will pick ~2–3k\n",
    "    num_leaves=256,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=80,\n",
    "    subsample=0.8,\n",
    "    subsample_freq=1,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.2,\n",
    "    reg_lambda=0.8,\n",
    "    extra_trees=True,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "oof_proba  = np.zeros((len(X), 7), dtype=np.float32)\n",
    "test_proba = np.zeros((len(X_test), 7), dtype=np.float32)\n",
    "fold_accs, best_iters = [], []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y), 1):\n",
    "    X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "    y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "    model = LGBMClassifier(**base_params)\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        eval_metric=\"multi_logloss\",\n",
    "        categorical_feature=cat_cols,\n",
    "        callbacks=[early_stopping(EARLY_STOP_ROUNDS, verbose=True)]\n",
    "    )\n",
    "\n",
    "    va_proba = model.predict_proba(X_va, num_iteration=model.best_iteration_)\n",
    "    oof_proba[va_idx] = va_proba\n",
    "    va_pred = model.classes_[va_proba.argmax(axis=1)]\n",
    "    acc = accuracy_score(y_va, va_pred)\n",
    "    fold_accs.append(acc)\n",
    "    best_iters.append(model.best_iteration_)\n",
    "    print(f\"Fold {fold}: acc={acc:.5f} | best_iter={model.best_iteration_}\")\n",
    "\n",
    "    test_proba += model.predict_proba(X_test, num_iteration=model.best_iteration_) / N_FOLDS\n",
    "\n",
    "# OOF accuracy (proxy of train-time performance)\n",
    "oof_pred = model.classes_[oof_proba.argmax(axis=1)]\n",
    "oof_acc = accuracy_score(y, oof_pred)\n",
    "print(f\"\\nCV mean acc: {np.mean(fold_accs):.5f} ± {np.std(fold_accs):.5f}\")\n",
    "print(f\"OOF acc    : {oof_acc:.5f}\")\n",
    "print(f\"Best iters : {best_iters} | mean={int(np.mean(best_iters))}\")\n",
    "\n",
    "# ===================== Build submission from averaged folds =====================\n",
    "final_pred = model.classes_[test_proba.argmax(axis=1)].astype(int)\n",
    "submission = pd.DataFrame({\"Id\": test[\"Id\"], \"Cover_Type\": final_pred})\n",
    "\n",
    "csv_path = \"submission.csv\"\n",
    "zip_path = \"submission.csv.zip\"\n",
    "submission.to_csv(csv_path, index=False)\n",
    "print(f\"\\n✅ Fichier CSV créé: {csv_path} | shape={submission.shape}\")\n",
    "\n",
    "with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "    z.write(csv_path, arcname=os.path.basename(csv_path))\n",
    "print(f\"✅ Archive ZIP créée: {zip_path}\")\n",
    "\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e960c0-588d-4382-a87c-3b570ef06d57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
